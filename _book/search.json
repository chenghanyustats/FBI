[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Model-based Statistics",
    "section": "",
    "text": "Welcome\nThis book offers an introduction to statistics built around a model based view of data and uncertainty. Rather than treating descriptive statistics, probability, and inference as separate topics, we will return again and again to a simple organizing idea:\n\\[\nY_i = \\text{Model with random noises}\n\\]\nIn words, we assume that each observation \\(Y_i\\) can be described by\nEven the basic problem of estimating a single population mean fits into this framework: it is the case where \\(f(x_i)\\) is a constant. More complex tasks such as comparing groups or fitting a regression line simply enrich the form of \\(f(x)\\) and the information available in \\(x\\).\nThis model based perspective is the backbone of the entire book. It allows us to present classical frequentist methods, simulation based methods, and Bayesian methods as three complementary ways to answer the same inferential questions.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Model-based Statistics",
    "section": "",
    "text": "information about the unit \\(x_i\\) (such as group membership or a predictor),\na systematic part \\(f(x_i)\\) that links \\(x_i\\) to the typical value of \\(Y_i\\),\na random error term \\(\\varepsilon_i\\) that captures variability we do not explain.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#three-ways-to-think-about-inference",
    "href": "index.html#three-ways-to-think-about-inference",
    "title": "Model-based Statistics",
    "section": "Three ways to think about inference",
    "text": "Three ways to think about inference\nFor almost every inferential problem in this book, you will see three approaches side by side.\n\nDistribution-based methods\nWe treat the parameters in the model as fixed but unknown quantities and study the long run behavior of procedures. This leads to familiar ideas such as standard errors, confidence intervals, hypothesis tests, and p values. Distribution based methods use mathematical results about sampling distributions, such as the Central Limit Theorem and the \\(t\\) distribution.\nSimulation-based methods\nWe use the computer to mimic what could have happened under repeated sampling. Bootstrap methods approximate the sampling distribution of statistics directly from the data. Permutation and randomization tests approximate the distribution of test statistics under a null hypothesis. Simulation provides intuition for frequentist ideas and is often easier to extend to new situations than purely algebraic formulas.\nBayesian methods\nWe treat the unknown parameters in the model as random quantities and specify prior distributions that represent our initial beliefs or information. The data update these priors through the likelihood to produce posterior distributions. From the posterior we obtain point estimates, credible intervals, and probability statements about parameters and predictions that have a direct and intuitive interpretation.\n\nThe same model \\(Y_i = f(x_i) + \\varepsilon_i\\) underlies all three perspectives. What differs is how we treat the unknown parts of the model and how we quantify uncertainty.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#what-is-different-about-this-book",
    "href": "index.html#what-is-different-about-this-book",
    "title": "Model-based Statistics",
    "section": "What is different about this book",
    "text": "What is different about this book\nMany introductory statistics texts focus almost entirely on distribution based frequentist methods and mention simulation or Bayesian ideas only briefly, if at all. This book aims to be different in several ways.\n\nModel based from the start\nOne sample problems, two sample comparisons, analysis of variance, and regression are all presented as special cases of a common model based framework. This makes it easier to see connections between topics and to transition to more advanced modeling.\nFrequentist and Bayesian reasoning at the introductory level\nBayesian thinking is introduced early in simple settings, such as proportions and means, using graphical and computational tools rather than heavy algebra. Students see both confidence intervals and credible intervals, and they learn how to interpret each clearly.\nSimulation as a first class tool\nBootstrap intervals and permutation tests are not add ons at the end of the course. They are used throughout to build intuition for sampling variability, to check the behavior of classical methods, and to extend inference to situations where standard formulas do not apply.\nComparisons across methods\nFor key inferential tasks, such as inference for one mean, differences in means, proportions, and simple regression, the book presents distribution based, simulation based, and Bayesian approaches in a common structure. Each method is described in terms of its assumptions, interpretation, and strengths and limitations. Students see how the answers are similar, how they differ, and why.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#who-this-book-is-for",
    "href": "index.html#who-this-book-is-for",
    "title": "Model-based Statistics",
    "section": "Who this book is for",
    "text": "Who this book is for\nThis book is designed for\n\nstudents in a first course in statistics or data analysis who want a conceptually rich and modern introduction,\ninstructors who wish to integrate simulation and Bayesian ideas into an introductory course without giving up core frequentist content,\nreaders in fields such as the social sciences, health sciences, and data science who want a unified model based view of statistical inference.\n\nThe mathematical level assumes familiarity with high school algebra. Calculus is not required, although some optional sections sketch connections for interested readers. Programming experience is not required too.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#using-this-book-in-different-courses",
    "href": "index.html#using-this-book-in-different-courses",
    "title": "Model-based Statistics",
    "section": "Using this book in different courses",
    "text": "Using this book in different courses\nThe book is intentionally more comprehensive than a typical one term course. It is written to support several different paths:\n\na short quarter course that emphasizes data, visualization, and a gentle introduction to inference,\na one semester course that focuses mainly on frequentist methods, supported by simulation and brief Bayesian examples,\na one semester course that uses frequentist methods as a starting point and emphasizes Bayesian reasoning,\na two semester sequence in which a first term covers frequentist and simulation based inference and a second term develops Bayesian modeling more deeply.\n\nIn the next section, “Recommended paths,” we outline concrete chapter selections for these different uses and indicate which sections are core, which are extensions, and which are more advanced.\nThroughout the book, recurring visual conventions and section labels will help you recognize which material is frequentist, which is simulation based, which is Bayesian, and which is more advanced. This is intended to make the text flexible for instructors while giving students a coherent and connected experience.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "part1.html",
    "href": "part1.html",
    "title": "Statistics and Data",
    "section": "",
    "text": "Big ideas in this Part\nWhat is Statistics? What is Data Science? What are data? I bet you have heard about data science quite often these days. In fact, data science is a quite new buzzword that was not that popular early 21th century. Even now I don’t think there is a formal and clear definition of data science. In my opinion, data science is such a broad area and subject that anything techniques related to data could be viewed as a part of data science, and this book is by no means a comprehensive data science book covering every aspect of dealing with data.\nWhile data science is hot and fancy, statistics is a dull and old word that has been used for centuries. Believe or not, by the 18th century the term statistics is used to describe the systematic collection of demographic and economic data by state 1, and in mathematics, statistics, or be more formally statistical inference 2, is the process of using data analysis to infer properties of a population. Without doubt statistics plays an important role in lots of aspects of data no matter what data science is and how data science evolves.\nThis part, Statistics and Data (Chapters 1 through 7), builds the foundation for everything that follows. We introduce what data are, why we collect them, how we summarize them, and how model based thinking helps us make reliable conclusions from data. Across Chapters 1 to 7, we build a foundation for doing statistics as learning about unknowns from data under uncertationty using statistical model.\nMoreover, we also learn about the computing software we will be using for the rest of the book. When I was a college student, I learned statistics with some paper and pens, doing all the calculations by hand or a calculator. We won’t do that anymore and you should not because every company or institution is doing statistical analysis using computer software, whatever that is.",
    "crumbs": [
      "Statistics and Data"
    ]
  },
  {
    "objectID": "part1.html#footnotes",
    "href": "part1.html#footnotes",
    "title": "Statistics and Data",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/History_of_statistics↩︎\nhttps://en.wikipedia.org/wiki/Statistical_inference↩︎",
    "crumbs": [
      "Statistics and Data"
    ]
  },
  {
    "objectID": "part1-01-stats.html",
    "href": "part1-01-stats.html",
    "title": "1  Statistics as a Science of Data",
    "section": "",
    "text": "1.1 What is statistics?\nThe first question we ask in this book is “What is Statistics?”\nStatistics can be defined in a variety of ways, and there doesn’t seem to be one definition that describes it best. In everyday language, people use the word statistics in two common ways.\nFigure 1.1: Example of statistics as numeric records: Michael Jordan’s career statistics. Source: https://www.nba.com/stats/player/893/career",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics as a Science of Data</span>"
    ]
  },
  {
    "objectID": "part1-01-stats.html#what-is-statistics",
    "href": "part1-01-stats.html#what-is-statistics",
    "title": "1  Statistics as a Science of Data",
    "section": "",
    "text": "Statistics as numeric records.\nA news story might report unemployment statistics, or a sports site might list a player’s career statistics. For example, Figure 1.1 below shows Michael Jordan’s career statistics from his time in the NBA.\n\n\n\nStatistics as a discipline.\nIn this book, statistics is a way of thinking that uses data to learn about what we are interested while explicitly accounting for uncertainty.\n\n\n\n\n\n\n\nNoteA practical definition we will use\n\n\n\nThe American Statistical Association describes statistics as the science of learning from data and of measuring, controlling, and communicating uncertainty. 1\nThis definition matches the main theme of this textbook: we learn from data by building and using statistical models.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics as a Science of Data</span>"
    ]
  },
  {
    "objectID": "part1-01-stats.html#difference-between-statistics-and-data-science",
    "href": "part1-01-stats.html#difference-between-statistics-and-data-science",
    "title": "1  Statistics as a Science of Data",
    "section": "1.2 Difference between Statistics and Data Science",
    "text": "1.2 Difference between Statistics and Data Science\n\n Data Science \nBecause of their shared attributes, many find it hard to differentiate between statistics and data science. The tweets below poke fun at the lack of clarity surrounding the definition of data science/data scientists (Figure 1.6).\n\n\n\n\n\n\nFigure 1.4\n\n\n\n\n\n\n\n\n\nFigure 1.5\n\n\n\n\n\n\n\n\n\nFigure 1.6: Tweets about what Data Science is\n\n\n\nA more formal definition of data science can be found on Investopedia. This site defines Data Science as a field of applied mathematics and statistics that provides useful information based on large amounts of complex data or big data. Although this definition is helpful for understanding data science, Dan Ariely, a famous behavioral economist at Duke, joked about their use of the term big data in his tweet below (Figure 1.7).\n\n\n\n\n\n\nFigure 1.7: Professor Ariely on Big Data\n\n\n\nMore information can be gathered about the differences between these two fields from looking at the courses offered in the Statistics Department at UC Santa Cruz, my alma mater. From Figure 1.8 below, one can see that statistics primarily focuses on data analysis, methods and models.\n\n\n\n\n\n\n\n\n\nFigure 1.8: Courses offered by the Department of Statistics at UC Santa Cruz. Source: https://courses.engineering.ucsc.edu/courses/department/24\n\n\n\n\n\nThis statistics department, in particular, doesn’t talk a lot about data collection, organization, data presentation or data visualization. In typical statistics departments, there isn’t much instruction or research done on data collection, cleaning, storage, database management, and data visualization. Because statistics continues to focus on data analysis and modeling, Data Science now addresses these other processes that statistics passes over. The data science process includes the collection, organization, analysis, interpretation and presentation of data (Figure 1.9). Although statistics does not focus on these concepts, they are encompassed within the field of data science.\n\n\n\n\n\n\n\n\nFigure 1.9: The data science process created at Harvard by Joe Blitzstein and Hanspeter Pfister",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics as a Science of Data</span>"
    ]
  },
  {
    "objectID": "part1-01-stats.html#what-will-we-learn-in-this-course",
    "href": "part1-01-stats.html#what-will-we-learn-in-this-course",
    "title": "1  Statistics as a Science of Data",
    "section": "1.3 What Will We Learn In this Course?",
    "text": "1.3 What Will We Learn In this Course?\nBelow the main topics of this book are listed in the order in which they will be covered.\n\n\n\n\n\n\n\n\n\nWe do touch data collection and data visualization and data summary, but we will spend most of our time talking about probability and statistical inference methods that are circled on the list above. This book focuses on the statistical methods for analyzing data.\nIn summary, we will learn useful information\n\nabout the population we are interested in\nfrom our sample data\nthrough statistical inferential methods, including estimation and testing\n\n\n\n\n\n\n\n\n\nFigure 1.10: Illustration of obtaining sample data from a population\n\n\n\n\n\nDon’t worry if you have no idea of these terms. These are what we will discuss throughout the book, and I’ll explain each term in detail.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics as a Science of Data</span>"
    ]
  },
  {
    "objectID": "part1-02-data.html",
    "href": "part1-02-data.html",
    "title": "2  Data: Meaning, Collection, and Types",
    "section": "",
    "text": "2.1 What data mean in statistics\nIn Chapter 1, we described statistics as a science of data. This chapter takes the next step by clarifying what we mean by data and how data enter statistical work.\nIn everyday language, people use the word data to mean facts, numbers, or information. In statistics, data have a more specific meaning.\nData are recorded values, collected with a purpose, that represent information about individuals, objects, or events of interest.\nTwo details matter immediately.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data: Meaning, Collection, and Types</span>"
    ]
  },
  {
    "objectID": "part1-03-r.html",
    "href": "part1-03-r.html",
    "title": "3  ready foR data",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ready fo*R* data</span>"
    ]
  },
  {
    "objectID": "part1-04-py.html",
    "href": "part1-04-py.html",
    "title": "4  Prepare Yourself for data",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>*P*repare *Y*ourself for data</span>"
    ]
  },
  {
    "objectID": "part1-05-graphics.html",
    "href": "part1-05-graphics.html",
    "title": "5  Data Visualization",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "part1-06-numerics.html",
    "href": "part1-06-numerics.html",
    "title": "6  Data Numerics",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Numerics</span>"
    ]
  },
  {
    "objectID": "part1-07-model.html",
    "href": "part1-07-model.html",
    "title": "7  Model-based Thinking",
    "section": "",
    "text": "7.1 Setup\nA statistical model is a way to describe how our data might have been generated.\nFirst, we say what random quantities we care about. These include:\nthe random variables we can actually observe (like test scores in our sample), and\nsometimes random quantities we cannot observe directly but imagine could exist (like future scores or population values).\nSecond, we specify a probability distribution (or a family of possible distributions) that could describe these observable random variables. For example, we might assume scores are roughly Normal with some unknown mean and variance.\nThird, we identify the unknown pieces of that distribution, called parameters (such as the mean μ and standard deviation σ). These are the parts we want to learn about from the data.\nFinally, sometimes we also put a probability distribution on the parameters themselves. This is what we do in Bayesian statistics: we treat the parameters as random and describe our prior beliefs about them with a distribution.\nWhen we treat the unknown parameter(s) θ as random, we think of:\nthe distribution of the data “indexed by θ” as\nthe conditional distribution of the data given θ.\nIn other words, we imagine: “If the true parameter value were θ, how would the data behave?”\nIn this chapter we take a first step toward a central idea of this book:\nWe will use a simple equation to represent this idea,\n\\[\nY_i = f(x_i) + \\epsilon_i,\n\\]\nand we will look at pictures of real and simulated data to see why this viewpoint is useful.\nBefore we write down any formulas for inference, we want to build intuition for what this model based view means in practice.\nIn this chapter we will use R and a few packages for visualization and data manipulation.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model-based Thinking</span>"
    ]
  },
  {
    "objectID": "part1-07-model.html#data-units-variables-and-variation",
    "href": "part1-07-model.html#data-units-variables-and-variation",
    "title": "7  Model-based Thinking",
    "section": "7.2 Data, units, variables, and variation",
    "text": "7.2 Data, units, variables, and variation\nBefore we talk about models, we recall some key ideas from earlier chapters.\n\nA unit is one object or individual in the study (a person, a school, a machine, a plot of land).\nA variable records some characteristic for each unit (height, weight, test score, treatment group).\nA dataset is a collection of measurements on one or more variables for many units.\nVariation means that not all units have the same values. Even within a single group, people’s test scores or hours of sleep are not identical.\n\nModels are about describing and explaining this variation in a structured way.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model-based Thinking</span>"
    ]
  },
  {
    "objectID": "part1-07-model.html#story-how-much-do-students-sleep",
    "href": "part1-07-model.html#story-how-much-do-students-sleep",
    "title": "7  Model-based Thinking",
    "section": "7.3 Story: how much do students sleep",
    "text": "7.3 Story: how much do students sleep\nImagine that you want to understand how much undergraduate students at your university sleep on school nights.\nYou cannot measure every student, so you take a random sample of \\(n = 60\\) students and record the number of hours they slept last night.\nWe will simulate such a dataset to illustrate the ideas. In practice, this would be replaced with real data.\n\nn &lt;- 60\n\n# Simulate a \"true\" population: Normal with mean 7, sd 1, truncated to [3, 10]\nsleep_pop &lt;- rnorm(100000, mean = 7, sd = 1)\nsleep_pop &lt;- pmin(pmax(sleep_pop, 3), 10)\n\n# Draw a sample of 60 students from this population\nsleep_sample &lt;- sample(sleep_pop, size = n, replace = FALSE)\n\nsleep_df &lt;- tibble(\n  id   = 1:n,\n  sleep_hours = sleep_sample\n)\n\nhead(sleep_df)\n\n# A tibble: 6 × 2\n     id sleep_hours\n  &lt;int&gt;       &lt;dbl&gt;\n1     1        6.25\n2     2        7.43\n3     3        6.45\n4     4        8.40\n5     5        6.72\n6     6        6.93\n\n\nA simple visualization of the sample might be a dotplot or histogram.\n\n\n\n\n\n\n\n\nFigure 7.1: A dotplot of hours of sleep for 60 sampled students.\n\n\n\n\n\nIn the plot, each dot represents one student. We see variation:\n\nsome students report around 5 hours,\nmany are between 6 and 8 hours,\na few report 9 or more hours.\n\nThe plot alone does not answer questions like:\n\nWhat is the typical number of hours slept in this population\nHow much do individuals vary around that typical value\nIf we repeated the sampling, would we see a very different picture\n\nTo answer these questions, we introduce a simple model.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model-based Thinking</span>"
    ]
  },
  {
    "objectID": "part1-07-model.html#a-simple-model-for-one-mean",
    "href": "part1-07-model.html#a-simple-model-for-one-mean",
    "title": "7  Model-based Thinking",
    "section": "7.4 A simple model for one mean",
    "text": "7.4 A simple model for one mean\nLet \\(Y_i\\) be the hours of sleep for student \\(i\\). We want to describe the population using a single number, the mean hours of sleep, which we denote by \\(\\mu\\).\nA model based way to write this is\n\\[\nY_i = \\mu + \\epsilon_i,\n\\]\nwhere\n\n\\(\\mu\\) is the population mean hours of sleep,\n\\(\\epsilon_i\\) is the deviation of student \\(i\\) from that mean.\n\nWe often assume that the errors \\(\\epsilon_i\\) have mean zero and some spread, and that they follow a distribution that is roughly symmetric. One common choice is a Normal model.\nTo connect this idea to the plot, we can draw a smooth curve that represents a possible population distribution for hours of sleep, centered at the sample mean and with spread similar to the data.\n\n\n\n\n\n\n\n\nFigure 7.2: Observed sample (histogram and rug plot) with a smooth density curve representing a possible model for the population distribution.\n\n\n\n\n\nIn this plot:\n\nThe bars and rug show the observed sample.\nThe solid curve shows a smooth estimate of the sample density.\nThe dashed curve shows a Normal distribution with mean \\(\\hat{\\mu}\\) and standard deviation \\(\\hat{\\sigma}\\) estimated from the data.\n\nThe curves are models: simple mathematical descriptions of a process that could have generated data like this. The dots and bars are data: one realization from that process.\nIn our model based view:\n\nthe curve represents the model,\neach observation represents one realization from that model.\n\nOur goal in statistical inference will be to use the observed data to learn about the underlying model, for example the value of \\(\\mu\\).",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model-based Thinking</span>"
    ]
  },
  {
    "objectID": "part1-07-model.html#many-possible-samples-from-the-same-model",
    "href": "part1-07-model.html#many-possible-samples-from-the-same-model",
    "title": "7  Model-based Thinking",
    "section": "7.5 Many possible samples from the same model",
    "text": "7.5 Many possible samples from the same model\nIn practice you only see one sample, but the idea of a model is that it can generate many possible samples.\nTo make this idea concrete, suppose we take the Normal model with mean \\(\\hat{\\mu}\\) and standard deviation \\(\\hat{\\sigma}\\) from the previous section as our current best guess for the population. We can ask a computer to simulate several new samples of size \\(n = 60\\) from this model.\n\n\n\n\n\n\n\n\nFigure 7.3: Several simulated samples from the same model for hours of sleep. Each panel shows a different sample of 60 students.\n\n\n\n\n\nEach panel shows a sample of 60 pseudo students generated from the same model. Notice that:\n\nthe overall shape in each panel is similar (most values around 7, fewer at the extremes),\nthe exact bars and sample means differ from panel to panel,\nsome samples look more lumpy than others.\n\nAll of these samples come from the same underlying model. The model is stable; the samples vary.\nThis visualizes the idea that:\n\nData are one realization from an underlying data generating process.\n\nYour actual dataset is like one of these panels. You do not know which one in advance, and you never see the others, but thinking in terms of a model helps you reason about what would happen in repeated samples and how much variation to expect.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model-based Thinking</span>"
    ]
  },
  {
    "objectID": "part1-07-model.html#extending-the-model-adding-groups",
    "href": "part1-07-model.html#extending-the-model-adding-groups",
    "title": "7  Model-based Thinking",
    "section": "7.6 Extending the model: adding groups",
    "text": "7.6 Extending the model: adding groups\nThe previous example treated all students as exchangeable, with a single mean \\(\\mu\\).\nNow imagine that you record not only hours of sleep, but also whether each student is in a STEM major or a non STEM major. You might suspect that students in different majors have different sleep patterns.\nWe can capture this idea by letting the typical hours of sleep depend on a group indicator.\nLet\n\n\\(x_i = 0\\) for non STEM majors,\n\\(x_i = 1\\) for STEM majors.\n\nWe write the model\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i.\n\\]\nHere\n\n\\(\\beta_0\\) is the mean hours of sleep for non STEM majors,\n\\(\\beta_1\\) is the difference in mean sleep between STEM and non STEM majors,\n\\(\\epsilon_i\\) again captures individual deviations from the group mean.\n\nTo illustrate, we simulate a dataset with two groups.\n\nn_stem    &lt;- 35\nn_nonstem &lt;- 45\n\n# \"True\" group means and common sd for illustration\nmu_nonstem_true &lt;- 7.2\nmu_stem_true    &lt;- 6.8\nsigma_true      &lt;- 0.9\n\nsleep_groups_df &lt;- tibble(\n  major = c(rep(\"Non-STEM\", n_nonstem), rep(\"STEM\", n_stem)),\n  x     = if_else(major == \"STEM\", 1, 0),\n  sleep_hours = c(\n    rnorm(n_nonstem, mean = mu_nonstem_true, sd = sigma_true),\n    rnorm(n_stem,    mean = mu_stem_true,    sd = sigma_true)\n  )\n)\n\nhead(sleep_groups_df)\n\n# A tibble: 6 × 3\n  major        x sleep_hours\n  &lt;chr&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 Non-STEM     0        7.18\n2 Non-STEM     0        6.63\n3 Non-STEM     0        6.59\n4 Non-STEM     0        6.84\n5 Non-STEM     0        6.42\n6 Non-STEM     0        6.85\n\n\nWe can visualize the data by group and overlay the group means implied by the linear model.\n\n\n(Intercept)           x \n  7.4023859  -0.5838058 \n\n\n\n\n\n\n\n\nFigure 7.4: Hours of sleep by major group. Points show individual students. Horizontal lines show group means from a simple linear model.\n\n\n\n\n\nThe horizontal line in each group represents the model’s group mean:\n\nNon STEM: \\(\\mu_{  ext{non STEM}} = \\beta_0\\),\nSTEM: \\(\\mu_{  ext{STEM}} = \\beta_0 + \\beta_1\\).\n\nThe points are the observed data, which fluctuate around these model summaries.\nThe question “Is there a difference between the two group means” becomes the model based question “Is \\(\\beta_1\\) equal to zero”\nLater, we will see that:\n\ntwo sample \\(t\\) tests,\nbootstrap intervals for \\(\\mu_{    ext{STEM}} - \\mu_{  ext{non STEM}}\\),\nBayesian models for group means,\n\nare all ways of learning about \\(\\beta_1\\) in this simple model.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model-based Thinking</span>"
    ]
  },
  {
    "objectID": "part1-07-model.html#adding-a-quantitative-predictor-simple-linear-regression",
    "href": "part1-07-model.html#adding-a-quantitative-predictor-simple-linear-regression",
    "title": "7  Model-based Thinking",
    "section": "7.7 Adding a quantitative predictor: simple linear regression",
    "text": "7.7 Adding a quantitative predictor: simple linear regression\nNow consider a quantitative predictor.\nImagine that for each student we record both hours of sleep \\(Y_i\\) and number of weekly hours spent working at a paid job, \\(x_i\\). Perhaps students who work more hours sleep less.\nA common model is:\n\\[\nY_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i.\n\\]\n\n\\(\\beta_0\\) is the intercept: the expected sleep when \\(x_i = 0\\) (conceptually, a baseline).\n\\(\\beta_1\\) is the slope: the expected change in sleep for each additional hour of work per week.\n\\(\\epsilon_i\\) is the unexplained variation.\n\nWe simulate such data for illustration.\n\nn_reg &lt;- 80\n\nwork_hours &lt;- runif(n_reg, min = 0, max = 25)  # weekly work hours\n\nbeta0_true &lt;- 7.5\nbeta1_true &lt;- -0.04   # small negative association\nsigma_reg  &lt;- 0.7\n\nsleep_reg_df &lt;- tibble(\n  work_hours  = work_hours,\n  sleep_hours = beta0_true + beta1_true * work_hours + rnorm(n_reg, mean = 0, sd = sigma_reg)\n)\n\nhead(sleep_reg_df)\n\n# A tibble: 6 × 2\n  work_hours sleep_hours\n       &lt;dbl&gt;       &lt;dbl&gt;\n1      14.9         6.06\n2       6.72        8.23\n3      21.3         7.00\n4       2.34        7.13\n5       1.28        7.24\n6      11.2         7.11\n\n\nWe fit a simple linear regression model and plot the data with the fitted line.\n\n\n(Intercept)  work_hours \n  7.2101235  -0.0271345 \n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 7.5: Sleep versus weekly work hours, with a fitted regression line from the model \\(Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\).\n\n\n\n\n\nHere again we see data (points) and a model (line). The points do not fall exactly on the line; that gap is the error term \\(\\epsilon_i\\). The parameters \\(\\beta_0\\) and \\(\\beta_1\\) describe the systematic part of the relationship between work and sleep.\nLater chapters will show how to interpret and perform inference on \\(\\beta_1\\) using frequentist, simulation based, and Bayesian methods.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model-based Thinking</span>"
    ]
  },
  {
    "objectID": "part1-07-model.html#the-role-of-epsilon-unexplained-variation-and-assumptions",
    "href": "part1-07-model.html#the-role-of-epsilon-unexplained-variation-and-assumptions",
    "title": "7  Model-based Thinking",
    "section": "7.8 The role of \\(\\epsilon\\): unexplained variation and assumptions",
    "text": "7.8 The role of \\(\\epsilon\\): unexplained variation and assumptions\nIn all of these examples, the term \\(\\epsilon_i\\) plays an important role:\n\nIt captures individual to individual variation that our model does not explain.\nIt allows the model to be flexible: not everyone with the same \\(x_i\\) has the same \\(Y_i\\).\n\nIn many classical models we will assume that:\n\nthe \\(\\epsilon_i\\) are independent,\nthey have mean zero,\nthey have the same variance \\(\\sigma^2\\) for all \\(i\\),\nand sometimes that they follow a Normal distribution.\n\nThese assumptions are simplifications that make it possible to develop mathematical results and to use standard procedures. Later chapters will:\n\nshow how to check these assumptions with residual plots and diagnostics,\nexplore nonparametric and robust methods when assumptions are doubtful,\nand show how Bayesian models can relax or modify some of these assumptions.\n\nFor now, it is enough to keep in mind:\n\nThe error term \\(\\epsilon_i\\) is where randomness lives.\nThe parameters in \\(f(x_i)\\) are what we want to learn about.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model-based Thinking</span>"
    ]
  },
  {
    "objectID": "part1-07-model.html#frequentist-simulation-based-and-bayesian-views-of-the-same-model",
    "href": "part1-07-model.html#frequentist-simulation-based-and-bayesian-views-of-the-same-model",
    "title": "7  Model-based Thinking",
    "section": "7.9 Frequentist, simulation based, and Bayesian views of the same model",
    "text": "7.9 Frequentist, simulation based, and Bayesian views of the same model\nThe equation\n\\[\nY_i = f(x_i) + \\epsilon_i\n\\]\nis the same in all three paradigms we will study:\n\nFrequentist view\n\nThe parameters inside \\(f\\) (such as \\(\\mu\\), \\(\\beta_0\\), \\(\\beta_1\\)) are fixed but unknown.\nThe randomness comes from the data (from the \\(\\epsilon_i\\)).\nWe study long run properties of procedures (confidence intervals, hypothesis tests).\n\nSimulation based view\n\nWe use the computer to mimic repeated sampling from the model or from the data.\nWe approximate the sampling distribution of statistics by resampling (bootstrap) or randomization.\nThis gives us approximate intervals and p values without relying entirely on formulas.\n\nBayesian view\n\nWe place a probability distribution (a prior) on the unknown parameters inside \\(f\\).\nThe data update this prior to a posterior distribution via the likelihood implied by the model.\nWe make probability statements about parameters and predictions based on the posterior.\n\n\nThe model itself does not change. What changes is:\n\nhow we treat the unknown parameters, and\nhow we define and measure uncertainty.\n\nLater, when we reach chapters such as “Inference for One Population Mean” and “Inference for Two Means,” you will see all three perspectives applied to the same model.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model-based Thinking</span>"
    ]
  },
  {
    "objectID": "part1-07-model.html#why-a-model-based-approach-is-helpful",
    "href": "part1-07-model.html#why-a-model-based-approach-is-helpful",
    "title": "7  Model-based Thinking",
    "section": "7.10 Why a model based approach is helpful",
    "text": "7.10 Why a model based approach is helpful\nThere are two main reasons we take this model based approach from the beginning.\n\n7.10.1 Unification of topics\nMany traditional courses present:\n\none sample \\(t\\) tests,\ntwo sample \\(t\\) tests,\nANOVA,\nsimple regression,\n\nas different procedures with different formulas and conditions. In this book, these all become special cases of linear models built from the same template \\(Y = f(x) + \\epsilon\\).\nThis helps you see connections:\n\nANOVA is regression with categorical predictors.\nTwo sample tests are ANOVA with two groups.\nRegression is an extension of these ideas, not a completely new topic.\n\n\n\n7.10.2 Better match to modern practice\nIn applied statistics and data science, analysts almost always begin by specifying a model for how the data might have been generated, then use software to fit the model and summarize uncertainty.\nBy thinking in terms of models from the start, you will be better prepared to:\n\nunderstand the output of statistical software,\nextend basic ideas to more complex models later,\nread research papers that use regression, generalized linear models, and Bayesian methods.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model-based Thinking</span>"
    ]
  },
  {
    "objectID": "part1-07-model.html#summary-and-looking-ahead",
    "href": "part1-07-model.html#summary-and-looking-ahead",
    "title": "7  Model-based Thinking",
    "section": "7.11 Summary and looking ahead",
    "text": "7.11 Summary and looking ahead\nIn this chapter we:\n\nintroduced the model based template \\(Y_i = f(x_i) + \\epsilon_i\\),\nsaw how one sample, two sample, and regression problems can all be written in this form,\nused graphics to distinguish between data (dots, bars, points) and models (curves, lines),\ndiscussed the role of the error term \\(\\epsilon_i\\) and basic assumptions,\nand previewed how frequentist, simulation based, and Bayesian methods all work with the same model but treat parameters and uncertainty differently.\n\nIn the next part of the book we will study randomness and data generating processes. We will learn about probability, random variables, and sampling distributions, which provide the foundation for understanding why our inferential methods work and how they relate to the model based view introduced here.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model-based Thinking</span>"
    ]
  },
  {
    "objectID": "part2.html",
    "href": "part2.html",
    "title": "Probability Model as Data Generating Process",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Probability Model as Data Generating Process"
    ]
  },
  {
    "objectID": "part2-01-prob.html",
    "href": "part2-01-prob.html",
    "title": "8  Probability: Meanings and Rules",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Randomness and Data Generating Process",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability: Meanings and Rules</span>"
    ]
  },
  {
    "objectID": "part2-02-dist.html",
    "href": "part2-02-dist.html",
    "title": "9  Probability Distributions",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Randomness and Data Generating Process",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "part2-03-sampling.html",
    "href": "part2-03-sampling.html",
    "title": "10  Sampling and Simulation",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Randomness and Data Generating Process",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Simulation</span>"
    ]
  },
  {
    "objectID": "part2-04-llnclt.html",
    "href": "part2-04-llnclt.html",
    "title": "11  Law of Large Numbers and Central Limit Theorem",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Randomness and Data Generating Process",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Law of Large Numbers and Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "part3.html",
    "href": "part3.html",
    "title": "Learning from Data",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Learning from Data"
    ]
  },
  {
    "objectID": "part3-01-inference.html",
    "href": "part3-01-inference.html",
    "title": "12  What Is Statistical Inference",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Learning from Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>What Is Statistical Inference</span>"
    ]
  },
  {
    "objectID": "part3-02-frequentist.html",
    "href": "part3-02-frequentist.html",
    "title": "13  Frequentist Reasoning",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Learning from Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Frequentist Reasoning</span>"
    ]
  },
  {
    "objectID": "part3-03-bayesian.html",
    "href": "part3-03-bayesian.html",
    "title": "14  Bayesian Reasoning",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Learning from Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bayesian Reasoning</span>"
    ]
  },
  {
    "objectID": "part3-04-compare.html",
    "href": "part3-04-compare.html",
    "title": "15  Distribution-based vs Simulation-based Inference",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Learning from Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Distribution-based vs Simulation-based Inference</span>"
    ]
  },
  {
    "objectID": "part4.html",
    "href": "part4.html",
    "title": "Inference for Models of Normal Data",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Inference for Models of Normal Data"
    ]
  },
  {
    "objectID": "part4-01-onemean.html",
    "href": "part4-01-onemean.html",
    "title": "16  Inference for One Population Mean",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Inference for Normal Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Inference for One Population Mean</span>"
    ]
  },
  {
    "objectID": "part4-02-twomean.html",
    "href": "part4-02-twomean.html",
    "title": "17  Comparing Two Groups",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Inference for Normal Data",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Comparing Two Groups</span>"
    ]
  },
  {
    "objectID": "part5.html",
    "href": "part5.html",
    "title": "Linear Models for Normal Data",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Linear Models for Normal Data"
    ]
  },
  {
    "objectID": "part5-01-slr.html",
    "href": "part5-01-slr.html",
    "title": "18  Simple Linear Regression",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Linear Models for Normal Data",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "part5-02-anova.html",
    "href": "part5-02-anova.html",
    "title": "19  One-Way Analysis of Variance",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Linear Models for Normal Data",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>One-Way Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "part5-03-mlr.html",
    "href": "part5-03-mlr.html",
    "title": "20  Multiple Linear Regression",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Linear Models for Normal Data",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "part5-04-twoway.html",
    "href": "part5-04-twoway.html",
    "title": "21  Two-Way ANOVA",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Linear Models for Normal Data",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Two-Way ANOVA</span>"
    ]
  },
  {
    "objectID": "part6.html",
    "href": "part6.html",
    "title": "Inference for Models of Binary Data",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Inference for Models of Binary Data"
    ]
  },
  {
    "objectID": "part6-01-oneproportion.html",
    "href": "part6-01-oneproportion.html",
    "title": "22  Inference for One Population Proportion",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Inference and Linear Models for Binary Data",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Inference for One Population Proportion</span>"
    ]
  },
  {
    "objectID": "part6-02-twoproportion.html",
    "href": "part6-02-twoproportion.html",
    "title": "23  Comparing Two Proportions",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Inference and Linear Models for Binary Data",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Comparing Two Proportions</span>"
    ]
  },
  {
    "objectID": "part6-03-logistic.html",
    "href": "part6-03-logistic.html",
    "title": "24  Binary Logistic Regression",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\nadd new words here\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Inference and Linear Models for Binary Data",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "a-r_prog.html",
    "href": "a-r_prog.html",
    "title": "Appendix A — R programming",
    "section": "",
    "text": "A.1 Arithmetic and Logical Operators\n2 + 3 / (5 * 4) ^ 2\n\n[1] 2.0075\n\n5 == 5.00\n\n[1] TRUE\n\n# 5 and 5L are of the same value too\n# 5 is of type double; 5L is integer\n5 == 5L\n\n[1] TRUE\n\ntypeof(5L)\n\n[1] \"integer\"\n\n!TRUE == FALSE\n\n[1] TRUE\nType coercion: When doing AND/OR comparisons, all nonzero values are treated as TRUE and 0 as FALSE.\n-5 | 0\n\n[1] TRUE\n\n1 & 1\n\n[1] TRUE\n\n2 | 0\n\n[1] TRUE",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#math-functions",
    "href": "a-r_prog.html#math-functions",
    "title": "Appendix A — R programming",
    "section": "A.2 Math Functions",
    "text": "A.2 Math Functions\nMath functions in R are built-in.\n\nsqrt(144)\n\n[1] 12\n\nexp(1)\n\n[1] 2.718282\n\nsin(pi/2)\n\n[1] 1\n\nlog(32, base = 2)\n\n[1] 5\n\nabs(-7)\n\n[1] 7",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#variables-and-assignment",
    "href": "a-r_prog.html#variables-and-assignment",
    "title": "Appendix A — R programming",
    "section": "A.3 Variables and Assignment",
    "text": "A.3 Variables and Assignment\nUse &lt;- to do assignment. Why\n\n## we create an object, value 5, \n## and call it x, which is a variable\nx &lt;- 5\nx\n\n[1] 5\n\n(x &lt;- x + 6)\n\n[1] 11\n\nx == 5\n\n[1] FALSE\n\nlog(x)\n\n[1] 2.397895",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#object-types",
    "href": "a-r_prog.html#object-types",
    "title": "Appendix A — R programming",
    "section": "A.4 Object Types",
    "text": "A.4 Object Types\ncharacter, double, integer and logical.\n\ntypeof(5)\n\n[1] \"double\"\n\n\n\ntypeof(5L)\n\n[1] \"integer\"\n\n\n\ntypeof(\"I_love_data_science!\")\n\n[1] \"character\"\n\n\n\ntypeof(1 &gt; 3)\n\n[1] \"logical\"\n\n\n\nis.double(5L)\n\n[1] FALSE",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#data-structure---vector",
    "href": "a-r_prog.html#data-structure---vector",
    "title": "Appendix A — R programming",
    "section": "A.5 Data Structure - Vector",
    "text": "A.5 Data Structure - Vector\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable defined previously is a scalar value, or in fact a (atomic) vector of length one.\n\n\nA.5.1 (Atomic) Vector\n\nTo create a vector, use c(), short for concatenate or combine.\nAll elements of a vector must be of the same type.\n\n\n(dbl_vec &lt;- c(1, 2.5, 4.5)) \n\n[1] 1.0 2.5 4.5\n\n(int_vec &lt;- c(1L, 6L, 10L))\n\n[1]  1  6 10\n\n## TRUE and FALSE can be written as T and F\n(log_vec &lt;- c(TRUE, FALSE, F))  \n\n[1]  TRUE FALSE FALSE\n\n(chr_vec &lt;- c(\"pretty\", \"girl\"))\n\n[1] \"pretty\" \"girl\"  \n\n\n\n## check how many elements in a vector\nlength(dbl_vec) \n\n[1] 3\n\n## check a compact description of \n## any R data structure\nstr(dbl_vec) \n\n num [1:3] 1 2.5 4.5\n\n\n\n\nA.5.2 Sequence of Numbers\n\nUse : to create a sequence of integers.\nUse seq() to create a sequence of numbers of type double with more options. \n\n\n(vec &lt;- 1:5) \n\n[1] 1 2 3 4 5\n\ntypeof(vec)\n\n[1] \"integer\"\n\n# a sequence of numbers from 1 to 10 with increment 2\n(seq_vec &lt;- seq(from = 1, to = 10, by = 2))\n\n[1] 1 3 5 7 9\n\ntypeof(seq_vec)\n\n[1] \"double\"\n\n\n\n# a sequence of numbers from 1 to 10\n# with 12 elements\nseq(from = 1, to = 10, length.out = 12)\n\n [1]  1.000000  1.818182  2.636364  3.454545  4.272727  5.090909  5.909091\n [8]  6.727273  7.545455  8.363636  9.181818 10.000000\n\n\n\n\nA.5.3 Operations on Vectors\n\nWe can do any operations on vectors as we do on a scalar variable (vector of length 1).\n\n\n# Create two vectors\nv1 &lt;- c(3, 8)\nv2 &lt;- c(4, 100) \n\n## All operations happen element-wisely\n# Vector addition\nv1 + v2\n\n[1]   7 108\n\n# Vector subtraction\nv1 - v2\n\n[1]  -1 -92\n\n\n\n# Vector multiplication\nv1 * v2\n\n[1]  12 800\n\n# Vector division\nv1 / v2\n\n[1] 0.75 0.08\n\nsqrt(v2)\n\n[1]  2 10\n\n\n\n\nA.5.4 Recycling of Vectors\n\nIf we apply arithmetic operations to two vectors of unequal length, the elements of the shorter vector will be recycled to complete the operations.  \n\n\nv1 &lt;- c(3, 8, 4, 5)\n# The following 2 operations are the same\nv1 * 2\n\n[1]  6 16  8 10\n\nv1 * c(2, 2, 2, 2)\n\n[1]  6 16  8 10\n\nv3 &lt;- c(4, 11)\nv1 + v3  ## v3 becomes c(4, 11, 4, 11) when doing the operation\n\n[1]  7 19  8 16\n\n\n\n\nA.5.5 Subsetting Vectors\n\nTo extract element(s) in a vector, we use a pair of brackets [] with element indexing.\nThe indexing starts with 1.\n\n\n\n\nv1\n\n[1] 3 8 4 5\n\nv2\n\n[1]   4 100\n\n## The 3rd element\nv1[3] \n\n[1] 4\n\n\n\n\nv1[c(1, 3)]\n\n[1] 3 4\n\nv1[1:2]\n\n[1] 3 8\n\n## extract all except a few elements\n## put a negative sign before the vector of \n## indices\nv1[-c(2, 3)] \n\n[1] 3 5",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#data-structure---factor",
    "href": "a-r_prog.html#data-structure---factor",
    "title": "Appendix A — R programming",
    "section": "A.6 Data Structure - Factor",
    "text": "A.6 Data Structure - Factor\n\nA vector of type factor can be ordered in a meaningful way.\nCreate a factor by factor(). It is a type of integer, not character. 😲 🙄\n\n\n## Create a factor from a character vector using function factor()\n(fac &lt;- factor(c(\"med\", \"high\", \"low\")))\n\n[1] med  high low \nLevels: high low med\n\ntypeof(fac)  ## The type is integer.\n\n[1] \"integer\"\n\nstr(fac)  ## The integers show the level each element in vector fac belongs to.\n\n Factor w/ 3 levels \"high\",\"low\",\"med\": 3 1 2\n\n\n\norder_fac &lt;- factor(c(\"med\", \"high\", \"low\"),\n                    levels = c(\"low\", \"med\", \"high\"))\nstr(order_fac)\n\n Factor w/ 3 levels \"low\",\"med\",\"high\": 2 3 1\n\n\n\nlevels(fac) ## Each level represents an integer, ordered from the vector alphabetically.\n\n[1] \"high\" \"low\"  \"med\"",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#data-structure---list",
    "href": "a-r_prog.html#data-structure---list",
    "title": "Appendix A — R programming",
    "section": "A.7 Data Structure - List",
    "text": "A.7 Data Structure - List\n\n\n\n\n\n\n\n\n\n\nLists are different from (atomic) vectors: Elements can be of any type, including lists. (Generic vectors)\nConstruct a list by using list().\n\n\n\n\n## a list of 3 elements of different types\nx_lst &lt;- list(idx = 1:3, \n              \"a\", \n              c(TRUE, FALSE))\n\n\n\n$idx\n[1] 1 2 3\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1]  TRUE FALSE\n\n\n\n\nstr(x_lst)\n\nList of 3\n $ idx: int [1:3] 1 2 3\n $    : chr \"a\"\n $    : logi [1:2] TRUE FALSE\n\nnames(x_lst)\n\n[1] \"idx\" \"\"    \"\"   \n\nlength(x_lst)\n\n[1] 3\n\n\n\n\n\nA.7.1 Subsetting a list\n\n\n Return an  element  of a list\n\n## subset by name (a vector)\nx_lst$idx  \n\n[1] 1 2 3\n\n## subset by indexing (a vector)\nx_lst[[1]]  \n\n[1] 1 2 3\n\ntypeof(x_lst$idx)\n\n[1] \"integer\"\n\n\n\n\n Return a  sub-list  of a list\n\n## subset by name (still a list)\nx_lst[\"idx\"]  \n\n$idx\n[1] 1 2 3\n\n## subset by indexing (still a list)\nx_lst[1]  \n\n$idx\n[1] 1 2 3\n\ntypeof(x_lst[\"idx\"])\n\n[1] \"list\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf list x is a train carrying objects, then x[[5]] is the object in car 5; x[4:6] is a train of cars 4-6.\n— @RLangTip, https://twitter.com/RLangTip/status/268375867468681216",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#data-structure---matrix",
    "href": "a-r_prog.html#data-structure---matrix",
    "title": "Appendix A — R programming",
    "section": "A.8 Data Structure - Matrix",
    "text": "A.8 Data Structure - Matrix\n\nA matrix is a two-dimensional analog of a vector with attribute dim.\nUse command matrix() to create a matrix.\n\n\n## Create a 3 by 2 matrix called mat\n(mat &lt;- matrix(data = 1:6, nrow = 3, ncol = 2)) \n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\ndim(mat); nrow(mat); ncol(mat)\n\n[1] 3 2\n\n\n[1] 3\n\n\n[1] 2\n\n\n\n# elements are arranged by row\nmatrix(data = 1:6, \n       nrow = 3, \n       ncol = 2, \n       byrow = TRUE) #&lt;&lt;\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\nattributes(mat)\n\n$dim\n[1] 3 2\n\n\n\nA.8.1 Row and Column Names\n\n\n\nmat\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n## assign row names and column names\nrownames(mat) &lt;- c(\"A\", \"B\", \"C\")\ncolnames(mat) &lt;- c(\"a\", \"b\")\nmat\n\n  a b\nA 1 4\nB 2 5\nC 3 6\n\n\n\n\nrownames(mat)\n\n[1] \"A\" \"B\" \"C\"\n\ncolnames(mat)\n\n[1] \"a\" \"b\"\n\nattributes(mat)\n\n$dim\n[1] 3 2\n\n$dimnames\n$dimnames[[1]]\n[1] \"A\" \"B\" \"C\"\n\n$dimnames[[2]]\n[1] \"a\" \"b\"\n\n\n\n\n\n\nA.8.2 Subsetting a Matrix\n\nUse the same indexing approach as vectors on rows and columns.\nUse comma , to separate row and column index.\nmat[2, 2] extracts the element of the second row and second column.\n\n\n\n\nmat\n\n  a b\nA 1 4\nB 2 5\nC 3 6\n\n## all rows and 2nd column\n## leave row index blank\n## specify 2 in coln index\nmat[, 2]\n\nA B C \n4 5 6 \n\n\n\n\n## 2nd row and all columns\nmat[2, ] \n\na b \n2 5 \n\n## The 1st and 3rd rows and the 1st column\nmat[c(1, 3), 1] \n\nA C \n1 3 \n\n\n\n\n\n\nA.8.3 Binding Matrices\n\ncbind() (binding matrices by adding columns)\nrbind() (binding matrices by adding rows)\nWhen matrices are combined by columns (rows), they should have the same number of rows (columns).\n\n\n\n\nmat\n\n  a b\nA 1 4\nB 2 5\nC 3 6\n\nmat_c &lt;- matrix(data = c(7,0,0,8,2,6), \n                nrow = 3, ncol = 2)\n## should have the same number of rows\ncbind(mat, mat_c)  \n\n  a b    \nA 1 4 7 8\nB 2 5 0 2\nC 3 6 0 6\n\n\n\n\nmat_r &lt;- matrix(data = 1:4, \n                nrow = 2, \n                ncol = 2)\n## should have the same number of columns\nrbind(mat, mat_r)  \n\n  a b\nA 1 4\nB 2 5\nC 3 6\n  1 3\n  2 4",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#data-structure---data-frame",
    "href": "a-r_prog.html#data-structure---data-frame",
    "title": "Appendix A — R programming",
    "section": "A.9 Data Structure - Data Frame",
    "text": "A.9 Data Structure - Data Frame\n\nA data frame is of type list of equal-length vectors, having a 2-dimensional structure.\nMore general than matrix: Different columns can have different types.\nUse data.frame() that takes named vectors as input “element”.\n\n\n\n\n## data frame w/ an dbl column named age\n## and char column named gender.\n(df &lt;- data.frame(age = c(19, 21, 40), \n                  gen = c(\"m\",\"f\", \"m\")))\n\n  age gen\n1  19   m\n2  21   f\n3  40   m\n\n## a data frame has a list structure\nstr(df)  \n\n'data.frame':   3 obs. of  2 variables:\n $ age: num  19 21 40\n $ gen: chr  \"m\" \"f\" \"m\"\n\n\n\n\n## must set column names\n## or they are ugly and non-recognizable\ndata.frame(c(19,21,40), c(\"m\",\"f\",\"m\")) \n\n  c.19..21..40. c..m....f....m..\n1            19                m\n2            21                f\n3            40                m\n\n\n\n\n\nA.9.1 Properties of data frames\nData frame has properties of matrix and list.\n\n\n\nnames(df)  ## df as a list\n\n[1] \"age\" \"gen\"\n\ncolnames(df)  ## df as a matrix\n\n[1] \"age\" \"gen\"\n\nlength(df) ## df as a list\n\n[1] 2\n\nncol(df) ## df as a matrix\n\n[1] 2\n\ndim(df) ## df as a matrix\n\n[1] 3 2\n\n\n\n\n## rbind() and cbind() can be used on df\ndf_r &lt;- data.frame(age = 10, \n                   gen = \"f\")\nrbind(df, df_r)\n\n  age gen\n1  19   m\n2  21   f\n3  40   m\n4  10   f\n\ndf_c &lt;- \n    data.frame(col = c(\"red\",\"blue\",\"gray\"))\n(df_new &lt;- cbind(df, df_c))\n\n  age gen  col\n1  19   m  red\n2  21   f blue\n3  40   m gray\n\n\n\n\n\n\nA.9.2 Subsetting a data frame\nCan use either list or matrix subsetting methods.\n\n\n\ndf_new\n\n  age gen  col\n1  19   m  red\n2  21   f blue\n3  40   m gray\n\n## Subset rows\ndf_new[c(1, 3), ]\n\n  age gen  col\n1  19   m  red\n3  40   m gray\n\n## select the row where age == 21\ndf_new[df_new$age == 21, ]\n\n  age gen  col\n2  21   f blue\n\n\n\n\n## Subset columns\n## like a list\ndf_new$age\n\n[1] 19 21 40\n\ndf_new[c(\"age\", \"gen\")]\n\n  age gen\n1  19   m\n2  21   f\n3  40   m\n\n## like a matrix\ndf_new[, c(\"age\", \"gen\")]\n\n  age gen\n1  19   m\n2  21   f\n3  40   m\n\n\n\n\n\ndf_new[c(1, 3), ]\n\n  age gen  col\n1  19   m  red\n3  40   m gray\n\nstr(df[\"age\"])  ## a data frame with one column\n\n'data.frame':   3 obs. of  1 variable:\n $ age: num  19 21 40\n\nstr(df[, \"age\"])  ## becomes a vector by default\n\n num [1:3] 19 21 40",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#special-objects",
    "href": "a-r_prog.html#special-objects",
    "title": "Appendix A — R programming",
    "section": "A.10 Special Objects",
    "text": "A.10 Special Objects",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#plotting",
    "href": "a-r_prog.html#plotting",
    "title": "Appendix A — R programming",
    "section": "A.11 Plotting",
    "text": "A.11 Plotting\n\nA.11.1 Scatter plot\n\n\n\nmtcars[1:15, 1:4]\n\n                    mpg cyl  disp  hp\nMazda RX4          21.0   6 160.0 110\nMazda RX4 Wag      21.0   6 160.0 110\nDatsun 710         22.8   4 108.0  93\nHornet 4 Drive     21.4   6 258.0 110\nHornet Sportabout  18.7   8 360.0 175\nValiant            18.1   6 225.0 105\nDuster 360         14.3   8 360.0 245\nMerc 240D          24.4   4 146.7  62\nMerc 230           22.8   4 140.8  95\nMerc 280           19.2   6 167.6 123\nMerc 280C          17.8   6 167.6 123\nMerc 450SE         16.4   8 275.8 180\nMerc 450SL         17.3   8 275.8 180\nMerc 450SLC        15.2   8 275.8 180\nCadillac Fleetwood 10.4   8 472.0 205\n\n\n\n\nplot(x = mtcars$mpg, y = mtcars$hp, \n     xlab  = \"Miles per gallon\", \n     ylab = \"Horsepower\", \n     main = \"Scatter plot\", \n     col = \"red\", \n     pch = 5, las = 1)\n\n\n\n\n\n\n\n\n\n\n\n\nA.11.2 Argument pch\n\nplot(x = 1:25, y = rep(1, 25), pch = 1:25, xlab = \"\", ylab = \"\", main = \"pch\", axes = FALSE)\naxis(1, at = 1:25, cex.axis = 0.5)\n\n\n\n\n\n\n\n\n\nThe default is pch = 1\n\n\n\nA.11.3 R Subplots\n\npar(mfrow = c(1, 2))\nplot(x = mtcars$mpg, y = mtcars$hp, xlab = \"mpg\")\nplot(x = mtcars$mpg, y = mtcars$weight, xlab = \"mpg\")\n\n\n\n\n\n\n\n\n\n\nA.11.4 Boxplot\n\npar(mar = c(4, 4, 0, 0))\nboxplot(mpg ~ cyl, \n        data = mtcars, \n        col = c(\"blue\", \"green\", \"red\"), \n        las = 1, \n        horizontal = TRUE,\n        xlab = \"Miles per gallon\", \n        ylab = \"Number of cylinders\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA.11.5 Histogram\nhist() decides the class intervals/with based on breaks. If not provided, R chooses one.\n\nhist(mtcars$wt, \n     breaks = 20, \n     col = \"#003366\", \n     border = \"#FFCC00\", \n     xlab = \"weights\", \n     main = \"Histogram of weights\",\n     las = 1)\n\n\n\n\n\n\n\n\n\nBesides color names, you can also use hex number to specify colors. Pretty handy.\n\n\n\nA.11.6 Barplot\n\n(counts &lt;- table(mtcars$gear)) \n\n\n 3  4  5 \n15 12  5 \n\n\n\nmy_bar &lt;- barplot(counts, \n                  main = \"Car Distribution\", \n                  xlab = \"Number of Gears\", \n                  las = 1)\ntext(x = my_bar, y = counts - 0.8, \n     labels = counts, \n     cex = 0.8)\n\n\n\n\n\n\n\n\n\n\nA.11.7 Pie chart\n\nPie charts are used for categorical variables, especially when we want to know percentage of each category.\nThe first argument is the frequency table, and you can add labels to each category.\n\n\n(percent &lt;- round(counts / sum(counts) * 100, 2))\n\n\n    3     4     5 \n46.88 37.50 15.62 \n\n(labels &lt;- paste0(3:5, \" gears: \", percent, \"%\"))\n\n[1] \"3 gears: 46.88%\" \"4 gears: 37.5%\"  \"5 gears: 15.62%\"\n\n\n\npie(x = counts, labels = labels,\n    main = \"Pie Chart\", \n    col = 2:4, \n    radius = 1)\n\n\n\n\n\n\n\n\n\n\nA.11.8 2D imaging\n\nThe image() function displays the values in a matrix using color.\n\n\nmatrix(1:30, 6, 5)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    7   13   19   25\n[2,]    2    8   14   20   26\n[3,]    3    9   15   21   27\n[4,]    4   10   16   22   28\n[5,]    5   11   17   23   29\n[6,]    6   12   18   24   30\n\nimage(matrix(1:30, 6, 5))\n\n\n\n\n\n\n\n\n\nlibrary(fields)\n\nLoading required package: spam\n\n\nSpam version 2.11-1 (2025-01-20) is loaded.\nType 'help( Spam)' or 'demo( spam)' for a short introduction \nand overview of this package.\nHelp for individual functions is also obtained by adding the\nsuffix '.spam' to the function name, e.g. 'help( chol.spam)'.\n\n\n\nAttaching package: 'spam'\n\n\nThe following objects are masked from 'package:base':\n\n    backsolve, forwardsolve\n\n\nLoading required package: viridisLite\n\n\n\nTry help(fields) to get started.\n\nstr(volcano)\n\n num [1:87, 1:61] 100 101 102 103 104 105 105 106 107 108 ...\n\nimage.plot(volcano)\n\n\n\n\n\n\n\n\n\nx &lt;- 10*(1:nrow(volcano))\ny &lt;- 10*(1:ncol(volcano))\nimage(x, y, volcano, col = hcl.colors(100, \"terrain\"), axes = FALSE)\ncontour(x, y, volcano, levels = seq(90, 200, by = 5),\n        add = TRUE, col = \"brown\")\naxis(1, at = seq(100, 800, by = 100))\naxis(2, at = seq(100, 600, by = 100))\nbox()\ntitle(main = \"Maunga Whau Volcano\", font.main = 4)\n\n\n\n\n\n\n\n\n\n\nA.11.9 3D scatter plot\n\nlibrary(scatterplot3d)\nscatterplot3d(x = mtcars$wt, y = mtcars$disp, z = mtcars$mpg, \n              xlab = \"Weights\", ylab = \"Displacement\", zlab = \"Miles per gallon\", \n              main = \"3D Scatter Plot\",\n              pch = 16, color = \"steelblue\")\n\n\n\n\n\n\n\n\n\n\nA.11.10 Perspective plot\n\n# Exaggerate the relief\nz &lt;- 2 * volcano      \n# 10 meter spacing (S to N)\nx &lt;- 10 * (1:nrow(z))   \n# 10 meter spacing (E to W)\ny &lt;- 10 * (1:ncol(z))   \npar(bg = \"slategray\")\npersp(x, y, z, theta = 135, phi = 30, col = \"green3\", scale = FALSE,\n      ltheta = -120, shade = 0.75, border = NA, box = FALSE)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#special-objects-1",
    "href": "a-r_prog.html#special-objects-1",
    "title": "Appendix A — R programming",
    "section": "A.12 Special Objects",
    "text": "A.12 Special Objects\n\nA.12.1 NA\n\nNA means Not Available, which is a logical constant of length 1 for a missing value indicator.\nEach type of vector has its own missing value. They all are reserved words.\nYou can always use NA and it will be converted to the correct type.\n\n\n\n\nNA            # logical\n\n[1] NA\n\nNA_integer_   # integer\n\n[1] NA\n\nNA_real_      # double\n\n[1] NA\n\nNA_character_ # character\n\n[1] NA\n\n\n\n\n\n## The NA in the vector x is NA_real_\nx &lt;- c(NA, 0, 1)\ntypeof(x)  \n\n[1] \"double\"\n\nis.na(x)\n\n[1]  TRUE FALSE FALSE\n\n\n\n\n\n\nA.12.2 NULL\n\nNULL represents the null object, an object representing nothing.\nNULL is a reserved word and can also be the product of importing data with unknown data type.\nNULL typically behaves like a vector of length 0.\n\n\n\n\ny &lt;- c(NA, NULL, \"\")\ny\n\n[1] NA \"\"\n\n## only first element is evaluated...\nis.null(y) \n\n[1] FALSE\n\n\n\n\n## a missing value is a value we don't know.\n## It is something.\nis.null(NA)\n\n[1] FALSE\n\nis.null(NULL)\n\n[1] TRUE\n\n# empty character is something, not nothing!\nis.null(\"\")\n\n[1] FALSE\n\n\n\n\n\n\nA.12.3 NaN, Inf, and -Inf\n\nIntegers have one special value: NA, while doubles have four: NA, NaN, Inf and -Inf.\nNaN means Not a Number.\nAll three special values NaN, Inf and -Inf can arise during division.\n\n\nc(-1, 0, 1) / 0\n\n[1] -Inf  NaN  Inf\n\n\n\nAvoid using ==. Use functions is.finite(), is.infinite(), and is.nan().\n\n\n\n\nis.finite(0)\n\n[1] TRUE\n\nis.nan(0/0)\n\n[1] TRUE\n\n\n\n\nis.infinite(7.8/1e-307)\n\n[1] FALSE\n\nis.infinite(7.8/1e-308)\n\n[1] TRUE\n\n\n\n\n\n\nA.12.4 Helper Functions\n\nNaN is a missing value too.\nThere should be something there, but it’s Not Available to us because of invalid operation.\n\n\n\n\n\n0\nInf\nNA\nNaN\n\n\n\n\nis.finite()\nv\n\n\n\n\n\nis.infinite()\n\nv\n\n\n\n\nis.na()\n\n\nv\nv\n\n\nis.nan()\n\n\n\nv\n\n\n\n\n\nA.12.5 NA, NULL, NaN comparison\n\n\n\nclass(NULL); class(NA); class(NaN)\n\n[1] \"NULL\"\n\n\n[1] \"logical\"\n\n\n[1] \"numeric\"\n\nNULL &gt; 5; NA &gt; 5; NaN &gt; 5\n\nlogical(0)\n\n\n[1] NA\n\n\n[1] NA\n\nlength(NULL); length(NA); length(NaN)\n\n[1] 0\n\n\n[1] 1\n\n\n[1] 1\n\n\n\n\n(vx &lt;- c(3, NULL, 5)); (vy &lt;- c(3, NA, 5)); (vz &lt;- c(3, NaN, 5))\n\n[1] 3 5\n\n\n[1]  3 NA  5\n\n\n[1]   3 NaN   5\n\nsum(vx)  # NULL isn't a problem cuz it doesn't exist\n\n[1] 8\n\nsum(vy)\n\n[1] NA\n\nsum(vy, na.rm = TRUE)\n\n[1] 8\n\nsum(vz)\n\n[1] NaN\n\nsum(vz, na.rm = TRUE)\n\n[1] 8",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#conditions",
    "href": "a-r_prog.html#conditions",
    "title": "Appendix A — R programming",
    "section": "A.13 Conditions",
    "text": "A.13 Conditions\n\n# The condition must evaluate to either TRUE or FALSE.\nif (condition) {\n  # code executed when condition is TRUE\n} else {\n  # code executed when condition is FALSE\n}\n\n\nif (c(TRUE, FALSE)) {}\n#&gt; Warning in if (c(TRUE, FALSE)) {: the condition has length &gt; 1 and only the\n#&gt; first element will be used\n#&gt; NULL\n\nif (NA) {}\n#&gt; Error in if (NA) {: missing value where TRUE/FALSE needed\n\n\nYou can use || (or) and && (and) to combine multiple logical expressions.\n\n\nif (cond1 || cond2) {\n  # code executed when condition is TRUE\n}\n\n\nThe basic If-else statement structure is that we write if then put a condition inside a pair of parenthesis, then use curly braces to wrap the code to be run when the condition is TRUE.\nIf we want to have the code to be run when the condition is FALSE, we add else and another pair of curly braces.\ncurly braces is not necessary if you just have one line of code to be run.\nThe condition must evaluate to either one TRUE or one FALSE.\nIf it’s a vector, you’ll get a warning message, and only the first element will be used.\nIf it’s an NA, you’ll get an error.\n\n\n\nif (this) {\n    # do that\n} else if (that) {\n    # do something else\n} else {\n    # \n}\n\n\nif (celsius &lt;= 0) {\n    \"freezing\"\n} else if (celsius &lt;= 10) {\n    \"cold\"\n} else if (celsius &lt;= 20) {\n   \"cool\"\n} else if (celsius &lt;= 30) {\n    \"warm\"\n} else {\n    \"hot\"\n}\n\n\nIf we want to have multiple conditions, we add the word else if.\nThe code below is an example of converting numerical data to categorical data, freezing, cold, warm and hot.\nIt’s not the best way to do conversion.\n\n\n\nIf you end up with a very long series of chained if-else statements, rewrite it!\n\n\n\n\n\n\nhttps://speakerdeck.com/jennybc/code-smells-and-feels\n\n\n\n\n\nIf you end up with a very long series of chained if-else statements, rewrite it! Don’t confuse readers and especially yourself.\nWe have a function called “get same data”.\nWhen you read the code in a linear fashion from top to bottom, you are falling down and down into conditions that were like a long time ago that you saw what you are actually checking.\nHere is another way to write the exactly the same function. ….. and now I am on the happy path. I get the data.\nSo if I open a file, and I know that something is gone wrong and checking all these things, I am much happier to be facing this than that!\nI think our brain cannot process too many layers. When we are trying to analyze so many layers, we just get lost.\n\n\n\n\n\n\n\nhttps://speakerdeck.com/jennybc/code-smells-and-feels\n\n\n\n\n\nSo there is no else, there is only if!\nBack, on the left, every if has an else.\nOn the right, we have no else. And this makes your code much more readable!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#functions",
    "href": "a-r_prog.html#functions",
    "title": "Appendix A — R programming",
    "section": "A.14 Functions",
    "text": "A.14 Functions\n\nWrite a function whenever you’ve copied and pasted your code more than twice.\nThree key steps/components:\n\npick a name for the function.\nlist the inputs, or arguments, to the function inside function.\nplace the code you have developed in body of the function.\n\n\n\nfunction_name &lt;- function(arg1, arg2, ...) {\n    ## body\n    return(something)\n}\n\n\nadd_number &lt;- function(a, b) {\n    c &lt;- a + b\n    return(c)\n}\n\nn1 &lt;- 9\nn2 &lt;- 18\nadd_number(n1, n2)\n\n[1] 27",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#loops",
    "href": "a-r_prog.html#loops",
    "title": "Appendix A — R programming",
    "section": "A.15 Loops",
    "text": "A.15 Loops\n\nA.15.1 for loops\n\n## Syntax\nfor (value in that) {\n    this\n}\n\n\nfor (i in 1:5) {\n    print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\nfor (x in c(\"My\", \"1st\", \"for\", \"loop\")) {\n    print(x)\n}\n\n[1] \"My\"\n[1] \"1st\"\n[1] \"for\"\n[1] \"loop\"\n\n\n\n\nA.15.2 while loops\n\nwhile (condition) {\n    # body\n}\n\n\nYou can rewrite any for loop as a while loop, but you can’t rewrite every while loop as a for loop.\n\n\nfor (i in seq_along(x)) {\n    # body\n}\n\n# Equivalent to\ni &lt;- 1\nwhile (i &lt;= length(x)) {\n    # body\n    i &lt;- i + 1 \n}\n\n\n\n\n\n\n\n\nWe find how many tries it takes to get 5 heads in a row:\n\n\n## a function that sample one from \"T\" or \"H\"\nflip &lt;- function() sample(c(\"T\", \"H\"), 1)\n\nflips &lt;- 0; nheads &lt;- 0\n\nwhile (nheads &lt; 5) {\n    if (flip() == \"H\") {\n        nheads &lt;- nheads + 1\n    } else {\n        nheads &lt;- 0\n    }\n    flips &lt;- flips + 1\n}\n\nflips\n\n[1] 230",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html",
    "href": "a-py_prog.html",
    "title": "Appendix B — Python Programming",
    "section": "",
    "text": "B.1 Arithmetic and Logical Operators\n2 + 3 / (5 * 4) ** 2\n\n2.0075\n\n5 == 5.00\n\nTrue\n\n5 == int(5)\n\nTrue\n\ntype(int(5))\n\n&lt;class 'int'&gt;\n\nnot True == False\n\nTrue\nbool() converts nonzero numbers to True and zero to False\n-5 | 0\n\n-5\n\n1 & 1\n\n1\n\nbool(2) | bool(0)\n\nTrue",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#math-functions",
    "href": "a-py_prog.html#math-functions",
    "title": "Appendix B — Python Programming",
    "section": "B.2 Math Functions",
    "text": "B.2 Math Functions\nNeed to import math library in Python.\n\nimport math\nmath.sqrt(144)\n\n12.0\n\nmath.exp(1)\n\n2.718281828459045\n\nmath.sin(math.pi/2)\n\n1.0\n\nmath.log(32, 2)\n\n5.0\n\nabs(-7)\n\n7\n\n\n\n# python comment",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#variables-and-assignment",
    "href": "a-py_prog.html#variables-and-assignment",
    "title": "Appendix B — Python Programming",
    "section": "B.3 Variables and Assignment",
    "text": "B.3 Variables and Assignment\n\nx = 5\nx\n\n5\n\nx = x + 6\nx\n\n11\n\nx == 5\n\nFalse\n\nmath.log(x)\n\n2.3978952727983707",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#object-types",
    "href": "a-py_prog.html#object-types",
    "title": "Appendix B — Python Programming",
    "section": "B.4 Object Types",
    "text": "B.4 Object Types\nstr, float, int and bool.\n\ntype(5.0)\n\n&lt;class 'float'&gt;\n\ntype(5)\n\n&lt;class 'int'&gt;\n\ntype(\"I_love_data_science!\")\n\n&lt;class 'str'&gt;\n\ntype(1 &gt; 3)\n\n&lt;class 'bool'&gt;\n\ntype(5) is float\n\nFalse",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#data-structure---lists",
    "href": "a-py_prog.html#data-structure---lists",
    "title": "Appendix B — Python Programming",
    "section": "B.5 Data Structure - Lists",
    "text": "B.5 Data Structure - Lists\n\nB.5.1 Lists\n\nPython has numbers and strings, but no built-in vector structure.\nTo create a sequence type of structure, we can use a list that can save several elements in an single object.\nTo create a list in Python, we use [].\n\n\nlst_num = [0, 2, 4] \nlst_num\n\n[0, 2, 4]\n\ntype(lst_num)\n\n&lt;class 'list'&gt;\n\nlen(lst_num)\n\n3\n\n\nList elements can have different types!\n\n\nB.5.2 Subsetting lists\n\nlst = ['data', 'math', 34, True]\nlst\n\n['data', 'math', 34, True]\n\n\n\nIndexing in Python always starts at 0!\n0: the 1st element\n\n\nlst\n\n['data', 'math', 34, True]\n\nlst[0]\n\n'data'\n\ntype(lst[0]) ## not a list\n\n&lt;class 'str'&gt;\n\n\n\n-1: the last element\n\n\nlst[-2]\n\n34\n\n\n\n[a:b]: the (a+1)-th to b-th elements\n\n\nlst[1:4]\n\n['math', 34, True]\n\ntype(lst[1:4]) ## a list\n\n&lt;class 'list'&gt;\n\n\n\n[a:]: elements from the (a+1)-th to the last\n\n\nlst[2:]\n\n[34, True]\n\n\nWhat does lst[0:1] return? Is it a list?\n\n\nB.5.3 Lists are mutable\n\nLists are changed in place!\n\n\nlst[1]\n\n'math'\n\nlst[1] = \"stats\"\nlst\n\n['data', 'stats', 34, True]\n\n\n\nlst[2:] = [False, 77]\nlst\n\n['data', 'stats', False, 77]\n\n\nIf we change any element value in a list, the list itself will be changed as well.\n\n\nB.5.4 List operations and methods list.method()\nThis is a common syntax in Python. We start with a Python object of some type, then type dot followed by any method specifically for this particular data type or structure for operations.\n\n## Concatenation\nlst_num + lst\n\n[0, 2, 4, 'data', 'stats', False, 77]\n\n\n\n## Repetition\nlst_num * 3 \n\n[0, 2, 4, 0, 2, 4, 0, 2, 4]\n\n\n\n## Membership\n34 in lst\n\nFalse\n\n\n\n## Appends \"cat\" to lst\nlst.append(\"cat\")\nlst\n\n['data', 'stats', False, 77, 'cat']\n\n\n\n## Removes and returns last object from list\nlst.pop()\n\n'cat'\n\nlst\n\n['data', 'stats', False, 77]\n\n\n\n## Removes object from list\nlst.remove(\"stats\")\nlst\n\n['data', False, 77]\n\n\n\n## Reverses objects of list in place\nlst.reverse()\nlst\n\n[77, False, 'data']",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#data-structure---tuples",
    "href": "a-py_prog.html#data-structure---tuples",
    "title": "Appendix B — Python Programming",
    "section": "B.6 Data Structure - Tuples",
    "text": "B.6 Data Structure - Tuples\n\nTuples work exactly like lists except they are immutable, i.e., they can’t be changed in place.\nTo create a tuple, we use ().\n\n\ntup = ('data', 'math', 34, True)\ntup\n\n('data', 'math', 34, True)\n\ntype(tup)\n\n&lt;class 'tuple'&gt;\n\nlen(tup)\n\n4\n\n\n\ntup[2:]\n\n(34, True)\n\ntup[-2]\n\n34\n\n\n\ntup[1] = \"stats\"  ## does not work!\n# TypeError: 'tuple' object does not support item assignment\n\n\ntup\n\n('data', 'math', 34, True)\n\n\n\nB.6.1 Tuples functions and methods\nLists have more methods than tuples because lists are more flexible.\n\n# Converts a list into tuple\ntuple(lst_num)\n\n(0, 2, 4)\n\n\n\n# number of occurance of \"data\"\ntup.count(\"data\")\n\n1\n\n\n\n# first index of \"data\"\ntup.index(\"data\")\n\n0",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#data-structure---dictionaries",
    "href": "a-py_prog.html#data-structure---dictionaries",
    "title": "Appendix B — Python Programming",
    "section": "B.7 Data Structure - Dictionaries",
    "text": "B.7 Data Structure - Dictionaries\n\nA dictionary consists of key-value pairs.\nA dictionary is mutable, i.e., the values can be changed in place and more key-value pairs can be added.\nTo create a dictionary, we use {\"key name\": value}.\nThe value can be accessed by the key in the dictionary.\n\n\ndic = {'Name': 'Ivy', 'Age': 7, 'Class': 'First'}\n\n\ndic['Age']\n\n7\n\n\n\ndic['age']  ## does not work\n\n\ndic['Age'] = 9\ndic['Class'] = 'Third'\ndic\n\n{'Name': 'Ivy', 'Age': 9, 'Class': 'Third'}\n\n\n\nB.7.1 Properties of dictionaries\n\nPython will use the last assignment!\n\n\ndic1 = {'Name': 'Ivy', 'Age': 7, 'Name': 'Liya'}\ndic1['Name']\n\n'Liya'\n\n\n\nKeys are unique and immutable.\nA key can be a tuple, but CANNOT be a list.\n\n\n## The first key is a tuple!\ndic2 = {('First', 'Last'): 'Ivy Lee', 'Age': 7}\ndic2[('First', 'Last')]\n\n'Ivy Lee'\n\n\n\n## does not work\ndic2 = {['First', 'Last']: 'Ivy Lee', 'Age': 7}\ndic2[['First', 'Last']]\n\n\n\nB.7.2 Disctionary methods\n\ndic\n\n{'Name': 'Ivy', 'Age': 9, 'Class': 'Third'}\n\n\n\n## Returns list of dictionary dict's keys\ndic.keys()\n\ndict_keys(['Name', 'Age', 'Class'])\n\n\n\n## Returns list of dictionary dict's values\ndic.values()\n\ndict_values(['Ivy', 9, 'Third'])\n\n\n\n## Returns a list of dict's (key, value) tuple pairs\ndic.items()\n\ndict_items([('Name', 'Ivy'), ('Age', 9), ('Class', 'Third')])\n\n\n\n## Adds dictionary dic2's key-values pairs in to dic\ndic2 = {'Gender': 'female'}\ndic.update(dic2)\ndic\n\n{'Name': 'Ivy', 'Age': 9, 'Class': 'Third', 'Gender': 'female'}\n\n\n\n## Removes all elements of dictionary dict\ndic.clear()\ndic\n\n{}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#python-data-structures-for-data-science",
    "href": "a-py_prog.html#python-data-structures-for-data-science",
    "title": "Appendix B — Python Programming",
    "section": "B.8 Python Data Structures for Data Science",
    "text": "B.8 Python Data Structures for Data Science\n\nPython built-in data structures are not specifically for data science.\nTo use more data science friendly functions and structures, such as array or data frame, Python relies on packages NumPy and pandas.\n\n\nB.8.1 Installing NumPy and pandas\nIn your RStudio project, run\n\nlibrary(reticulate)\nvirtualenv_create(\"myenv\")\n\nGo to Tools &gt; Global Options &gt; Python &gt; Select &gt; Virtual Environments\n\n\n\n\nYou may need to restart R session. Do it, and in the new R session, run\n\nlibrary(reticulate)\npy_install(c(\"numpy\", \"pandas\", \"matplotlib\"))\n\nRun the following Python code, and make sure everything goes well.\n\nimport numpy as np\nimport pandas as pd\nv1 = np.array([3, 8])\nv1\ndf = pd.DataFrame({\"col\": ['red', 'blue', 'green']})\ndf",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#pandas",
    "href": "a-py_prog.html#pandas",
    "title": "Appendix B — Python Programming",
    "section": "B.9 Pandas",
    "text": "B.9 Pandas\npandas is a Python library that provides data structures, manipulation and analysis tools for data science.\n\nimport numpy as np\nimport pandas as pd\n\n\nB.9.1 Pandas series from a list\n\n# import pandas as pd\na = [1, 7, 2]\ns = pd.Series(a)\nprint(s)\n\n0    1\n1    7\n2    2\ndtype: int64\n\n\n\nprint(s[0])\n\n1\n\n\n\n## index used as naming \ns = pd.Series(a, index = [\"x\", \"y\", \"z\"])\nprint(s)\n\nx    1\ny    7\nz    2\ndtype: int64\n\n\n\nprint(s[\"y\"])\n\n7\n\n\n\n\nB.9.2 Pandas series from a dictionary\n\ngrade = {\"math\": 99, \"stats\": 97, \"cs\": 66}\ns = pd.Series(grade)\nprint(s)\n\nmath     99\nstats    97\ncs       66\ndtype: int64\n\n\n\ngrade = {\"math\": 99, \"stats\": 97, \"cs\": 66}\n\n## index used as subsetting \ns = pd.Series(grade, index = [\"stats\", \"cs\"])\nprint(s)\n\nstats    97\ncs       66\ndtype: int64\n\n\n\nHow do we create a named vector in R?\n\n\ngrade &lt;- c(\"math\" = 99, \"stats\" = 97, \"cs\" = 66)\n\n\n\nB.9.3 Pandas data frame\n\nCreate a data frame from a dictionary\n\n\ndata = {\"math\": [99, 65, 87], \"stats\": [92, 48, 88], \"cs\": [50, 88, 94]}\n\ndf = pd.DataFrame(data)\nprint(df) \n\n   math  stats  cs\n0    99     92  50\n1    65     48  88\n2    87     88  94\n\n\n\nRow and column names\n\n\ndf.index = [\"s1\", \"s2\", \"s3\"]\ndf.columns = [\"Math\", \"Stat\", \"CS\"]\ndf\n\n    Math  Stat  CS\ns1    99    92  50\ns2    65    48  88\ns3    87    88  94\n\n\n\n\nB.9.4 Subsetting columns\n\nIn Python, [] returns Series, [[]] returns DataFrame!\nIn R, [] returns tibble/data frame, [[]] returns vector!\n\nBy Names\n\n## Series\ndf[\"Math\"]\n\ns1    99\ns2    65\ns3    87\nName: Math, dtype: int64\n\ntype(df[\"Math\"])\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\nBy Index\n\n# ## DataFrame\ndf[[\"Math\"]]\n\n    Math\ns1    99\ns2    65\ns3    87\n\ntype(df[[\"Math\"]])\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\n\ndf[[\"Math\", \"CS\"]]\n\n    Math  CS\ns1    99  50\ns2    65  88\ns3    87  94\n\n\n\nisinstance(df[[“Math”]], pd.DataFrame)\n\n\n\nB.9.5 Subsetting rows DataFrame.iloc\n\ninteger-location based indexing for selection by position\n\n\ndf \n\n    Math  Stat  CS\ns1    99    92  50\ns2    65    48  88\ns3    87    88  94\n\n\n\n## first row Series\ndf.iloc[0] \n\nMath    99\nStat    92\nCS      50\nName: s1, dtype: int64\n\n\n\n## first row DataFrame\ndf.iloc[[0]]\n\n    Math  Stat  CS\ns1    99    92  50\n\n\n\n## first 2 rows\ndf.iloc[[0, 1]]\n\n    Math  Stat  CS\ns1    99    92  50\ns2    65    48  88\n\n\n\n## 1st and 3rd row\ndf.iloc[[True, False, True]]\n\n    Math  Stat  CS\ns1    99    92  50\ns3    87    88  94\n\n\n\n\nB.9.6 Subsetting rows and columns DataFrame.iloc\n\ndf \n\n    Math  Stat  CS\ns1    99    92  50\ns2    65    48  88\ns3    87    88  94\n\n\n\n## (1, 3) row and (1, 3) col\ndf.iloc[[0, 2], [0, 2]]\n\n    Math  CS\ns1    99  50\ns3    87  94\n\n\n\n## all rows and 1st col\ndf.iloc[:, [True, False, False]]\n\n    Math\ns1    99\ns2    65\ns3    87\n\n\n\ndf.iloc[0:2, 1:3]\n\n    Stat  CS\ns1    92  50\ns2    48  88\n\n\n\n\nB.9.7 Subsetting rows and columns DataFrame.loc\nAccess a group of rows and columns by label(s)\n\ndf \n\n    Math  Stat  CS\ns1    99    92  50\ns2    65    48  88\ns3    87    88  94\n\n\n\ndf.loc['s1', \"CS\"]\n\n50\n\n\n\n## all rows and 1st col\ndf.loc['s1':'s3', [True, False, False]]\n\n    Math\ns1    99\ns2    65\ns3    87\n\n\n\ndf.loc['s2', ['Math', 'Stat']]\n\nMath    65\nStat    48\nName: s2, dtype: int64\n\n\n\n\nB.9.8 Obtaining a single cell value DataFrame.iat/ DataFrame.at\n\ndf \n\n    Math  Stat  CS\ns1    99    92  50\ns2    65    48  88\ns3    87    88  94\n\n\n\ndf.iat[1, 2]\n\n88\n\n\n\ndf.iloc[0].iat[1]\n\n92\n\n\n\ndf.at['s2', 'Stat']\n\n48\n\n\n\ndf.loc['s1'].at['Stat']\n\n92\n\n\n\n\nB.9.9 New columns DataFrame.insert and new rows pd.concat\n\ndf \n\n    Math  Stat  CS\ns1    99    92  50\ns2    65    48  88\ns3    87    88  94\n\n\n\ndf.insert(loc = 2, \n          column = \"Chem\", \n          value = [77, 89, 76])\ndf\n\n    Math  Stat  Chem  CS\ns1    99    92    77  50\ns2    65    48    89  88\ns3    87    88    76  94\n\n\n\ndf1 = pd.DataFrame({\n    \"Math\": 88, \n    \"Stat\": 99, \n    \"Chem\": 0, \n    \"CS\": 100\n    }, index = ['s4'])\n\n\npd.concat(objs = [df, df1])\n\n    Math  Stat  Chem   CS\ns1    99    92    77   50\ns2    65    48    89   88\ns3    87    88    76   94\ns4    88    99     0  100\n\n\n\n\npd.concat(objs = [df, df1], \n          ignore_index = True)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#numpy",
    "href": "a-py_prog.html#numpy",
    "title": "Appendix B — Python Programming",
    "section": "B.10 NumPy",
    "text": "B.10 NumPy\n\nB.10.1 NumPy for arrays/matrices\nNumPy is used to work with arrays/matrices.\n\nThe array object in NumPy is called ndarray.\nUse array() to create an array.\n\n\nrange(0, 5, 1) # a seq of number from 0 to 4 with increment of 1\n\nrange(0, 5)\n\nlist(range(0, 5, 1))\n\n[0, 1, 2, 3, 4]\n\n\n\nimport numpy as np\narr = np.array(range(0, 5, 1)) ## One-dim array \narr\n\narray([0, 1, 2, 3, 4])\n\ntype(arr)\n\n&lt;class 'numpy.ndarray'&gt;\n\n\n\n\nB.10.2 1D array (vector) and 2D array (matrix)\n\nnp.arange: Efficient way to create a one-dim array of sequence of numbers\n\n\nnp.arange(2, 5)\n\narray([2, 3, 4])\n\nnp.arange(6, 0, -1)\n\narray([6, 5, 4, 3, 2, 1])\n\n\n\n2D array\n\n\nnp.array([[1, 2, 3], [4, 5, 6]])\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\n\nnp.array([[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]])\n\narray([[[1, 2, 3],\n        [4, 5, 6]],\n\n       [[1, 2, 3],\n        [4, 5, 6]]])\n\n\n\n\nB.10.3 np.reshape()\n\narr2 = np.arange(8).reshape(2, 4)\narr2\n\narray([[0, 1, 2, 3],\n       [4, 5, 6, 7]])\n\narr2.shape  \n\n(2, 4)\n\n\n\narr2.ndim\n\n2\n\n\n\narr2.size\n\n8\n\n\n\n\nB.10.4 Stacking arrays\n\na = np.array([1, 2, 3, 4]).reshape(2, 2)\nb = np.array([5, 6, 7, 8]).reshape(2, 2)\n\nnp.vstack((a, b))\n\narray([[1, 2],\n       [3, 4],\n       [5, 6],\n       [7, 8]])\n\n\n\nnp.hstack((a, b))\n\narray([[1, 2, 5, 6],\n       [3, 4, 7, 8]])",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#plotting",
    "href": "a-py_prog.html#plotting",
    "title": "Appendix B — Python Programming",
    "section": "B.11 Plotting",
    "text": "B.11 Plotting\nmatplotlib.markers\n\npch = np.array(['.', ',', 'o', 'v', '^', '&lt;', '&gt;', '1', '2', '3', '4', '8', 's', 'p', 'P', '*', 'h', 'H', '+', 'x', 'X', 'D', 'd', '|', '_'])\n#all types of maker\npch_len = pch.shape[0]\nx = np.array([i for i in range(1, pch_len+1)])\ny = np.ones(pch_len)\n\n\nplt.figure(0)\nfor i in range(0, pch_len):\n    plt.plot(x[i],y[i],pch[i])\n\n\n\n\n\n\n\n\n\nB.11.1 Scatterplot\n\n\nCode\nmtcars = pd.read_csv('./data/mtcars.csv')\nmtcars.iloc[0:15,0:4]\n\n\n     mpg  cyl   disp   hp\n0   21.0    6  160.0  110\n1   21.0    6  160.0  110\n2   22.8    4  108.0   93\n3   21.4    6  258.0  110\n4   18.7    8  360.0  175\n5   18.1    6  225.0  105\n6   14.3    8  360.0  245\n7   24.4    4  146.7   62\n8   22.8    4  140.8   95\n9   19.2    6  167.6  123\n10  17.8    6  167.6  123\n11  16.4    8  275.8  180\n12  17.3    8  275.8  180\n13  15.2    8  275.8  180\n14  10.4    8  472.0  205\n\n\n\nimport matplotlib.pyplot as plt\nplt.scatter(x = mtcars.mpg, y = mtcars.hp, color = \"r\")\nplt.xlabel(\"Miles per gallon\")\nplt.ylabel(\"Horsepower\")\nplt.title(\"Scatter plot\")\n\n\n\n\n\n\n\n\n\n\nB.11.2 Subplots\nThe command plt.scatter() is used for creating one single plot. If multiple subplots are wanted in one single call, one can use the format\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.scatter(x, y)\nax2.plot(x, y)\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.scatter(x = mtcars.mpg, y = mtcars.hp)\nax2.scatter(x = mtcars.hp, y = mtcars.disp)\n\n\n\n\n\n\n\n\n\nCheck Creating multiple subplots using plt.subplots for more details.\n\n\n\nB.11.3 Boxplot\n\n\nCode\ncyl_index = np.sort(np.unique(np.array(mtcars.cyl)))\ncyl_shape = cyl_index.shape[0]\ncyl_list = []\nfor i in range (0, cyl_shape):\n    cyl_list.append(np.array(mtcars[mtcars.cyl == cyl_index[i]].mpg))\n\n\n\nplt.boxplot(cyl_list, vert=False, tick_labels=[4, 6, 8])\n\n{'whiskers': [&lt;matplotlib.lines.Line2D object at 0x30f85c050&gt;, &lt;matplotlib.lines.Line2D object at 0x30f85d5b0&gt;, &lt;matplotlib.lines.Line2D object at 0x30f85e630&gt;, &lt;matplotlib.lines.Line2D object at 0x30f85e900&gt;, &lt;matplotlib.lines.Line2D object at 0x30f85f890&gt;, &lt;matplotlib.lines.Line2D object at 0x30f85fb60&gt;], 'caps': [&lt;matplotlib.lines.Line2D object at 0x30f85d7f0&gt;, &lt;matplotlib.lines.Line2D object at 0x30f85daf0&gt;, &lt;matplotlib.lines.Line2D object at 0x30f85ebd0&gt;, &lt;matplotlib.lines.Line2D object at 0x30f85eed0&gt;, &lt;matplotlib.lines.Line2D object at 0x30f85fe30&gt;, &lt;matplotlib.lines.Line2D object at 0x30f8a8140&gt;], 'boxes': [&lt;matplotlib.lines.Line2D object at 0x30f848c80&gt;, &lt;matplotlib.lines.Line2D object at 0x30f85e330&gt;, &lt;matplotlib.lines.Line2D object at 0x30f85f5f0&gt;], 'medians': [&lt;matplotlib.lines.Line2D object at 0x30f85ddc0&gt;, &lt;matplotlib.lines.Line2D object at 0x30f85f110&gt;, &lt;matplotlib.lines.Line2D object at 0x30f8a8410&gt;], 'fliers': [&lt;matplotlib.lines.Line2D object at 0x30f85e090&gt;, &lt;matplotlib.lines.Line2D object at 0x30f85f380&gt;, &lt;matplotlib.lines.Line2D object at 0x30f8a8740&gt;], 'means': []}\n\nplt.xlabel(\"Miles per gallon\")\nplt.ylabel(\"Number of cylinders\")\n\n\n\n\n\n\n\n\n\n\nB.11.4 Histogram\n\nplt.hist(mtcars.wt, \n         bins = 19, \n         color=\"#003366\",\n         edgecolor=\"#FFCC00\")\nplt.xlabel(\"weights\")\nplt.title(\"Histogram of weights\")\n\n\n\n\n\n\n\n\n\n\nB.11.5 Barplot\n\ncount_py = mtcars.value_counts('gear')\ncount_py\n\ngear\n3    15\n4    12\n5     5\nName: count, dtype: int64\n\n\n\nplt.bar(count_py.index, count_py)\nplt.xlabel(\"Number of Gears\")\nplt.title(\"Car Distribution\")\n\n\n\n\n\n\n\n\n\n\nB.11.6 Pie chart\n\npercent = round(count_py / sum(count_py) * 100, 2)\ntexts = [str(percent.index[k]) + \" gear \" + str(percent.array[k]) + \"%\" for k in range(0,3)]\n\n\nplt.pie(count_py, labels = texts, colors = ['r', 'g', 'b'])\n\n([&lt;matplotlib.patches.Wedge object at 0x30fa15b20&gt;, &lt;matplotlib.patches.Wedge object at 0x30f946330&gt;, &lt;matplotlib.patches.Wedge object at 0x30fa15cd0&gt;], [Text(0.10781885436251686, 1.0947031993394165, '3 gear 46.88%'), Text(-0.6111272563215624, -0.9146165735327998, '4 gear 37.5%'), Text(0.9701133907831904, -0.5185364105085978, '5 gear 15.62%')])\n\nplt.title(\"Pie Charts\")\n\n\n\n\n\n\n\n\n\n\nB.11.7 2D Imaging\nIn Python,\n\nmat_img = np.reshape(np.array(range(1,31)), [6,5], \"F\")\nmat_img\n\narray([[ 1,  7, 13, 19, 25],\n       [ 2,  8, 14, 20, 26],\n       [ 3,  9, 15, 21, 27],\n       [ 4, 10, 16, 22, 28],\n       [ 5, 11, 17, 23, 29],\n       [ 6, 12, 18, 24, 30]])\n\nplt.imshow(mat_img, cmap = 'Oranges')\n\n\n\n\n\n\n\n\n\nvolcano = pd.read_csv('./data/volcano.csv', index_col=0)\nx = 10*np.arange(1,volcano.shape[0]+1)\ny = 10*np.arange(1,volcano.shape[1]+1)\nX,Y = np. meshgrid(x,y)\nvt = volcano.transpose()\nprint(vt.shape)\n\n(61, 87)\n\nprint(X.shape)\n\n(61, 87)\n\nprint(Y.shape)\n\n(61, 87)\n\n\n\nfig, ax = plt.subplots()\nIM = ax.matshow(vt, alpha =1, cmap='terrain')\nCS = ax.contour(vt, levels=np.arange(90,200,5))\nax.clabel(CS, inline=True, fontsize=10)\nax.set_title('Maunga Whau Volcano')\n\n\n\n\n\n\n\n\n\n\nB.11.8 3D scatterplot\nIn Python,\n\nfig = plt.figure(figsize=(12, 12))\nax = fig.add_subplot(projection='3d')\n\nax.scatter(xs = mtcars.wt, ys = mtcars.disp, zs = mtcars.mpg)\nax.set_xlabel('Weights')\nax.set_ylabel(\"Displacement\")\nax.set_zlabel(\"Miles per gallon\")\nax.set_title(\"3D Scatter Plot\")\n\n\n\n\n\n\n\n\n\n\nB.11.9 Perspective plot\nIn Python,\n\nx = 10*np.arange(1,volcano.shape[0]+1)\ny = 10*np.arange(1,volcano.shape[1]+1)\nvt = volcano.transpose()\nZ = 10*vt\nX,Y = np. meshgrid(x,y)\n\nprint(Z.shape)\n\n(61, 87)\n\nprint(X.shape)\n\n(61, 87)\n\nprint(Y.shape)\n\n(61, 87)\n\n\n\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n# Plot the surface.\nax.plot_surface(X, Y, Z, cmap = 'Greens')",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#special-objects",
    "href": "a-py_prog.html#special-objects",
    "title": "Appendix B — Python Programming",
    "section": "B.12 Special Objects",
    "text": "B.12 Special Objects\nIn python, NA, NaN and NULL are not that distinguishable, comparing to R.\n\nNaN can be used as a numerical value on mathematical operations, while None cannot (or at least shouldn’t).\nNaN is a numeric value, as defined in IEEE 754 floating-point standard.\nNone is an internal Python type (NoneType) and would be more like “inexistent” or “empty” than “numerically invalid” in this context.\n\n\na = np.array([None, 0.9, 10])\ntype(a)\n\n&lt;class 'numpy.ndarray'&gt;\n\na == None\n\narray([ True, False, False])\n\nlen(a)\n\n3\n\nprint(type(a[0]))\n\n&lt;class 'NoneType'&gt;\n\n\n\nNone == None\n\nTrue\n\n'' == None\n\nFalse\n\n\n\na1 = np.array([-1,0,1])/0\n\n&lt;string&gt;:1: RuntimeWarning: divide by zero encountered in divide\n&lt;string&gt;:1: RuntimeWarning: invalid value encountered in divide\n\na1\n\narray([-inf,  nan,  inf])\n\n\n\nmath.isfinite(0)\n\nTrue\n\nmath.isnan(float(\"nan\"))\n\nTrue\n\npd.isna(float(\"nan\"))\n\nTrue\n\nnp.isnan(float(\"nan\"))\n\nTrue\n\n\n\nmath.isfinite(7.8/1e-307)\n\nTrue\n\nmath.isfinite(7.8/1e-308)\n\nFalse\n\n\n\ntype(None)\n\n&lt;class 'NoneType'&gt;\n\n\n\n## TypeError: '&gt;' not supported between instances of 'NoneType' and 'int'\nNone &gt; 5\n\n\n## TypeError: object of type 'NoneType' has no len()\nlen(None)\n\n\nfloat(\"NaN\") &gt; 5\n\nFalse\n\n\n\nv_none = np.array([3, None, 5])\nv_none\n\narray([3, None, 5], dtype=object)\n\nv_nan = np.array([3, float(\"NaN\"), 5])\nv_nan\n\narray([ 3., nan,  5.])\n\n\n\n# TypeError: unsupported operand type(s) for +: 'int' and 'NoneType'\nsum(v_none)\n\n\nsum(v_nan)\n\nnan",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#conditions",
    "href": "a-py_prog.html#conditions",
    "title": "Appendix B — Python Programming",
    "section": "B.13 Conditions",
    "text": "B.13 Conditions\n\nif condition:\n    # code executed when condition is true\nelse:\n    # code executed when condition is false\n\n\na = 5\nb = 20\nif a &gt; 4 or b &gt; 4:\n    print('a &gt; 4 or b &gt; 4')\n\na &gt; 4 or b &gt; 4\n\nif a &gt; 4 and b &gt; 4:\n    print('a &gt; 4 and b &gt; 4')\n\na &gt; 4 and b &gt; 4\n\n\n\nif (a &gt; 4) | (b &gt; 4):\n    print('a &gt; 4 or b &gt; 4')\n\na &gt; 4 or b &gt; 4\n\nif (a &gt; 4) & (b &gt; 4):\n    print('a &gt; 4 and b &gt; 4')\n\na &gt; 4 and b &gt; 4",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#multiple-conditions",
    "href": "a-py_prog.html#multiple-conditions",
    "title": "Appendix B — Python Programming",
    "section": "B.14 Multiple conditions",
    "text": "B.14 Multiple conditions\n\nif condition A:\n    # do that\nelif condition B:\n    # do something else\nelse:\n    # \n\n\nrd = np.random.randint(100)\nprint(rd)\n\n24\n\nif rd &lt;= 20:\n    print(\"rd &lt;= 20\")\nelif rd &gt; 20 and rd &lt;= 40:\n    print('rd &gt; 20 and rd &lt;= 40')\nelif rd &gt; 40 and rd &lt;= 60:\n    print('rd &gt; 40 and rd &lt;= 60')\nelif rd &gt; 60 and rd &lt;= 80:\n    print('rd &gt; 60 and rd &lt;= 80')\nelif rd &gt; 80 and rd &lt;= 100:\n    print('rd &gt; 80 and rd &lt;= 100')\n\nrd &gt; 20 and rd &lt;= 40",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#functions",
    "href": "a-py_prog.html#functions",
    "title": "Appendix B — Python Programming",
    "section": "B.15 Functions",
    "text": "B.15 Functions\n\ndef function_name(arg1, arg2, ...):\n    ## body\n    return(something)\n\n\ndef add_number(a, b):\n    c = a + b\n    return c\n\nn1 = 9\nn2 = 18\nadd_number(n1, n2)\n\n27",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#loops",
    "href": "a-py_prog.html#loops",
    "title": "Appendix B — Python Programming",
    "section": "B.16 Loops",
    "text": "B.16 Loops\n\nB.16.1 for loops\n\nPython\nfor value in that:\n    # do this\n\n\nfor i in range(5):\n    print('for', i)\n\nfor 0\nfor 1\nfor 2\nfor 3\nfor 4\n\n\n\nfor i in ['My', '1st', 'for', 'loop']:\n    print(i)\n\nMy\n1st\nfor\nloop\n\n\n\n\nB.16.2 while loops\n\nwhile (condition):\n    # do this\n\n\ni = 1\nwhile(i &lt; 5):\n    print('while',i)\n    i = i + 1\n\nwhile 1\nwhile 2\nwhile 3\nwhile 4\n\n\n\nnp.random.seed(86)\ndef flip():\n    return np.random.choice(['T','H'], 1)\n    \nflips = 0\nnheads = 0\n\nwhile(nheads &lt; 3):\n    if flip() == \"H\":\n        nheads += 1\n    else:\n        nheads = 0\n    flips += 1\n    \nflips\n\n9",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "part1.html#what-is-data",
    "href": "part1.html#what-is-data",
    "title": "Statistics and Data",
    "section": "What is data",
    "text": "What is data\nIn statistics, data are values of variables recorded on a set of units in a specific context.\n\nA unit (also called an observational unit) is the thing you measure, such as a person, a product, a school, a day, a county, or a patient visit.\nA variable is what you record on each unit, such as height, income, blood pressure, a survey response, a click, or a label such as treatment group.\n\nA useful habit is to treat every dataset as a sentence:\n\nWe measured what variables on which units, how and when were the measurements taken, and why were we collecting them.\n\n\nHow statistics views data\nStatistics does not treat data as a pile of numbers. It treats data as a realization of a data generating process, meaning that the values you see depend on how the world produced them and how you measured them.\nThis viewpoint matters because two datasets can look similar but carry very different information. For example, a dataset from a randomized experiment supports stronger causal conclusions than a dataset from an observational study, even if the summary statistics look the same.\n\n\nHow data are generated\nTo do statistics well, you need to know how data were generated. At an introductory level, the most important distinctions are these.\n\nPopulation versus sample\nA population is the full set of units you care about. A sample is the subset you actually observe. Statistics is largely about using a sample to learn about a population.\nObservational study versus experiment\nIn an observational study, you observe what happens without assigning treatments. In an experiment, you intervene and assign treatments, ideally using random assignment.\nMeasurement and recording\nData quality depends on how variables were measured, how missingness occurred, what was rounded or censored, and what errors were introduced during collection and storage.\n\nThese features of data generation are not minor details. They determine what questions you can answer, what assumptions are reasonable, and what kind of uncertainty you should report.\n\n\nWhere data come from\nModern datasets come from many sources. In this book we will repeatedly encounter data from:\n\nsurveys and questionnaires\ndesigned experiments\nobservational studies in medicine, social science, and business\nadministrative and operational records (for example, school records, hospital systems, public health reports)\nsensors and automated measurement systems\ndigital traces (for example, web activity, app usage, transaction logs)\n\nDifferent sources introduce different strengths and weaknesses, especially around bias, missing data, and measurement error.",
    "crumbs": [
      "Statistics and Data"
    ]
  },
  {
    "objectID": "part1.html#why-we-need-data",
    "href": "part1.html#why-we-need-data",
    "title": "Statistics and Data",
    "section": "Why we need data",
    "text": "Why we need data\nStatistics, machine learning, and data science all start with the same constraint: we cannot answer empirical questions by logic alone. We need data to learn how the world behaves.\nAt a high level, data are used for four recurring goals.\n\nDescription\nWhat happened in this dataset. What patterns are present.\nExplanation and comparison\nHow groups differ, and which variables are associated with an outcome.\nPrediction\nGiven some inputs, what value or category should we predict for a new unit.\nDecision making and policy\nWhat action should we take, given uncertainty and tradeoffs.\n\nEven when the final goal is prediction, statistics remains essential because it provides tools for understanding variability, quantifying uncertainty, and checking whether a model is stable enough to trust. This emphasis on uncertainty is central to the statistical perspective.5",
    "crumbs": [
      "Statistics and Data"
    ]
  },
  {
    "objectID": "part1.html#summarizing-data",
    "href": "part1.html#summarizing-data",
    "title": "Statistics and Data",
    "section": "Summarizing data",
    "text": "Summarizing data\nRaw data are often too detailed to think with directly. Summaries help you see structure and communicate evidence.\n\nWhy we summarize data\nWe summarize data in order to:\n\nreduce complexity without losing the main story\ncompare groups and detect differences\nidentify unusual observations and potential errors\nsupport later modeling choices (for example, choosing an outcome scale or deciding whether a linear trend is plausible)\n\n\n\nWhy summarize data this way\nNot all summaries are equally informative. Good summaries usually match the type of question you are asking and the type of variable you have.\n\nFor a quantitative variable, we often start with distribution summaries: center, spread, shape, and outliers.\nFor a categorical variable, we often start with counts and proportions.\nFor relationships between variables, we often start with plots that reveal association, trend, and variability.\n\nIn Chapters 5 and 6, you will learn a set of visual and numerical summaries that appear again and again in practice, and you will learn how to interpret them in context.\n\n\nHow summaries help us achieve statistical goals\nSummaries are not an end point. They support the full statistical workflow.\n\nThey help you clarify the question and define variables.\nThey guide model building by revealing patterns that a model should capture.\nThey help you check assumptions and diagnose problems.\nThey help you communicate conclusions clearly and honestly.",
    "crumbs": [
      "Statistics and Data"
    ]
  },
  {
    "objectID": "part1.html#model-based-thinking",
    "href": "part1.html#model-based-thinking",
    "title": "Statistics and Data",
    "section": "Model based thinking",
    "text": "Model based thinking\nThis book is built around model based thinking, meaning that we treat data analysis as an explicit attempt to connect three pieces:\n\na question about the world\na model that describes how the data could have been generated\na method for learning model parameters from data, with uncertainty\n\nAt the level of this book, the central template is:\n[ Y_i = f(x_i) + _i ]\nYou can read this as: each observation is a combination of a systematic part we want to learn, plus random variation we do not explain.\n\nWhy model based thinking helps\nModel based thinking helps because it forces you to state assumptions in a form that can be checked, criticized, and improved.\n\nIt connects descriptive summaries to inference by treating both as questions about a data generating process.\nIt clarifies what your target is: a parameter, a prediction, a comparison, or a causal effect.\nIt provides a common language that supports frequentist methods, simulation based methods, and Bayesian methods, even when their interpretations of probability differ.\n\nChapter 7 develops this idea carefully and shows how the same model template will organize the rest of the book.",
    "crumbs": [
      "Statistics and Data"
    ]
  },
  {
    "objectID": "part1.html#roadmap-for-chapters-1-through-7",
    "href": "part1.html#roadmap-for-chapters-1-through-7",
    "title": "Statistics and Data",
    "section": "Roadmap for Chapters 1 through 7",
    "text": "Roadmap for Chapters 1 through 7\nThis part is organized as a progression from broad orientation to concrete skills and then to the model based perspective.\n\nChapter 1: Science of Data and Data Science\nWhat data science is trying to do, where statistics fits, and what a full data analysis workflow looks like.\nChapter 2: Data, Studies, and Causality\nPopulations and samples, observational studies and experiments, and why study design matters for causal claims.\nChapter 3: Ready for R Data\nA practical start on working with data in a modern computing environment, including importing and organizing data.\nChapter 4: Prepare Yourself for Data\nData cleaning and basic data management habits that prevent analysis mistakes.\nChapter 5: Data Visualization\nGraphs as statistical arguments: how to reveal distributions, comparisons, and relationships.\nChapter 6: Data Numerics\nNumerical summaries that complement visualization and support later modeling.\nChapter 7: Model based Thinking\nThe unifying framework of the book: describing data with models, and using models to learn from data under uncertainty.\n\nBy the end of this part, you should be able to describe a dataset clearly, explain how it was produced, create informative summaries, and articulate a first model based description of variability.\n\n\n\n\n\n–&gt;\n\n–&gt;\n\n–&gt;",
    "crumbs": [
      "Statistics and Data"
    ]
  },
  {
    "objectID": "part1.html#statistics-and-data-science",
    "href": "part1.html#statistics-and-data-science",
    "title": "Statistics and Data",
    "section": "",
    "text": "What is data\nWhy do we need data\nHow do we summarize data\nWhat is model based statistical thinking",
    "crumbs": [
      "Statistics and Data"
    ]
  },
  {
    "objectID": "part1.html#big-ideas-in-this-part",
    "href": "part1.html#big-ideas-in-this-part",
    "title": "Statistics and Data",
    "section": "",
    "text": "What is data\nIn statistics, data are recorded values that represent information about individuals, objects, or events of interest. We will discuss:\n\nHow statistics views data as measurements with variability\nHow data are generated, including studies, surveys, and experiments\nWhere data come from, including observation, administrative records, sensors, and digital traces\n\n\n\nWhy we need data\nStatistics, machine learning, and data science all rely on data because data provide information about what we are curious about, and connect questions to evidence. We will discuss:\n\nWhy data are necessary for description, comparison, explanation, prediction, and decision making\nHow the purpose of the analysis shapes what data we need and how we collect them\n\n\n\nSummarizing data\nRaw data are often too detailed to interpret directly, so we use summaries to reveal patterns and variability. We will discuss:\n\nWhy we summarize data\nWhy we choose particular summaries, such as graphs and numerical summaries\nHow summaries support our goals, such as comparing groups or understanding relationships\n\n\n\nModel-based thinking\nThe book is model-based statistics. We will discuss:\n\nWhat it means to treat data as coming from a probabilistic model, or a data-generating process.\nHow models help us separate signal from noise, quantify uncertainty, and generalize beyond a sample",
    "crumbs": [
      "Statistics and Data"
    ]
  },
  {
    "objectID": "part1.html#chapter-roadmap",
    "href": "part1.html#chapter-roadmap",
    "title": "Statistics and Data",
    "section": "Chapter roadmap",
    "text": "Chapter roadmap\nChapter 1: Data, questions, and variability\nWhat data are, what makes a statistical question, and why variability is the key feature that statistics addresses.\nChapter 2: How data are produced\nPopulations and samples, observational studies, surveys, experiments, and how design choices affect what you can conclude.\nChapter 3: Describing one variable\nGraphs and numerical summaries for a single variable, with emphasis on interpreting center, spread, and unusual features.\nChapter 4: Relationships between variables\nTwo variable summaries, group comparisons, association, and how to interpret relationships without overclaiming causation.\nChapter 5: Probability as a model for data\nProbability as a language for describing random variation, including distributions and the meaning of parameters.\nChapter 6: Sampling and randomness\nSampling behavior, sampling distributions, and why statistics can learn from incomplete data.\nChapter 7: A first look at inference through models\nHow we use models to learn about unknown parameters, assess uncertainty, and connect data summaries to conclusions.\n\n\n\n\n\n\n\n\n–&gt;\n\n\n\n–&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;\n\n–&gt;\n\n–&gt;",
    "crumbs": [
      "Statistics and Data"
    ]
  },
  {
    "objectID": "part1-01-stats.html#section",
    "href": "part1-01-stats.html#section",
    "title": "1  Statistics as a Science of Data",
    "section": "1.4 ",
    "text": "1.4 \nStatistics can be divided into two parts: descriptive statistics and inferential statistics (statistical inference). Before doing inferential statistics, we usually learn to understand our data because we cannot use appropriate methods, do the analysis and interpret the result correctly without fully understanding our data. Descriptive statistics discusses how to describe and summarize data by frequency/count tables, graphics/visualization or some important numerical measures.\n\n\n\n\n\n\n\n\nFigure 1.11: Branches of statistics",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics as a Science of Data</span>"
    ]
  },
  {
    "objectID": "part1-01-stats.html#descriptive-vs-inferential-statistics",
    "href": "part1-01-stats.html#descriptive-vs-inferential-statistics",
    "title": "1  Statistics as a Science of Data",
    "section": "1.4 Descriptive vs Inferential Statistics",
    "text": "1.4 Descriptive vs Inferential Statistics\nStatistics can be divided into two parts: descriptive statistics and inferential statistics (statistical inference). Before doing inferential statistics, we usually learn to understand our data because we cannot use appropriate methods, do the analysis and interpret the result correctly without fully understanding our data. Descriptive statistics discusses how to describe and summarize data by frequency/count tables, graphics/visualization or some important numerical measures.\n\n\n\n\n\n\n\n\nFigure 1.11: Branches of statistics",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics as a Science of Data</span>"
    ]
  },
  {
    "objectID": "part1-01-stats.html#data-are-numbers-with-context",
    "href": "part1-01-stats.html#data-are-numbers-with-context",
    "title": "1  Statistics as a Science of Data",
    "section": "1.2 Data are numbers with context",
    "text": "1.2 Data are numbers with context\nA key idea for modern statistics is that data are not just numbers. Data can be numbers, labels, images, text, and other records with a context such as how they were produced, what they measure, and what they represent in the real world. Cobb and Moore famously summarized this as “data are numbers with a context.”2\nWhen you look at a dataset, you should be able to answer:\n\nWho or what subject was measured? (people, schools, products, days, patients)\nWhat attributes or characteristics of the subject were recorded? (height, income, blood pressure, response time)\nHow were the data generated? (survey, observational study, experiment, administrative records)",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics as a Science of Data</span>"
    ]
  },
  {
    "objectID": "part1-01-stats.html#populations-samples-and-statistics",
    "href": "part1-01-stats.html#populations-samples-and-statistics",
    "title": "1  Statistics as a Science of Data",
    "section": "1.3 Populations, samples, and statistics",
    "text": "1.3 Populations, samples, and statistics\nStatistics is often about learning something about a population using a sample.\n\nA population is the full set of individuals or situations you care about.\nA sample is a subset of the target population.\n\nUsually, the data we have is from a subset of the target population, and hence we call the data sample data. When the data include all subjects in the population, such data set is called census data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.2: Illustration of obtaining sample data from a population\n\n\n\n\n\nA statistic is any numerical summary computed from the sample (for example, a sample mean). A parameter is a number that describes the population or the data generating process (for example, a population mean).",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics as a Science of Data</span>"
    ]
  },
  {
    "objectID": "part1-01-stats.html#how-are-data-generated",
    "href": "part1-01-stats.html#how-are-data-generated",
    "title": "1  Statistics as a Science of Data",
    "section": "1.4 How are data generated?",
    "text": "1.4 How are data generated?\nData are produced by a process, and that process determines what kinds of conclusions are reasonable.\nCommon data sources include:\n\nSurveys and questionnaires We ask questions and record responses. Survey design affects bias and variability.\nObservational studies We record variables as they naturally occur. Observational data can reveal association but do not automatically justify cause and effect claims.\nExperiments We actively assign a treatment or condition. Random assignment is the key tool for supporting causal conclusions.\nAdministrative and digital records These include logs, transactions, medical records, sensors, and digital traces. These datasets can be large, but they still require careful thinking about measurement and selection.\n\nA useful phrase is data generating process: the real world mechanism that produces the values we observe. In a model-based approach, we build a probability model as an approximation to that process.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics as a Science of Data</span>"
    ]
  },
  {
    "objectID": "part1-01-stats.html#statistics-and-data-science",
    "href": "part1-01-stats.html#statistics-and-data-science",
    "title": "1  Statistics as a Science of Data",
    "section": "1.5 Statistics and data science",
    "text": "1.5 Statistics and data science\nStatistics and data science overlap heavily. Both aim to learn from data, and both use modeling.\nWiki lists a more formal definition of statistics in Figure 1.3 below.\n\n\n\n\n\n\n\n\nFigure 1.3: More formal definition of statistics. Source: https://en.wikipedia.org/wiki/Statistics\n\n\n\n\n\nStatistics is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data. Without doubt, statistics is a discipline dealing with data. However, in typical statistics departments, there isn’t much instruction or research done on data collection, cleaning, storage, database management, and data visualization. Because statistics continues to focus on data analysis and modeling, Data Science now addresses these other processes that statistics passes over.\nA common way to describe the difference is emphasis.\n\nStatistics emphasizes study design, uncertainty quantification, and inference from data to a broader target (a population, a process, or future outcomes).\nData science includes those statistical goals but also emphasizes data acquisition, data management, computing, and deployment of models in real systems.\n\nA helpful reference point is the National Academies report on undergraduate data science, which treats data science as an emerging discipline related to, but not identical to, statistics and computer science.3\nThis book is not a complete data science handbook. It focuses on the part that statistics has historically done best and that modern data work still needs: model-based reasoning with uncertainty.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics as a Science of Data</span>"
    ]
  },
  {
    "objectID": "part1-01-stats.html#describing-data-vs-learning-from-data",
    "href": "part1-01-stats.html#describing-data-vs-learning-from-data",
    "title": "1  Statistics as a Science of Data",
    "section": "1.6 Describing data vs learning from data",
    "text": "1.6 Describing data vs learning from data\nA good analysis usually starts with descriptive summaries and then moves toward inference.\n\nDescriptive statistics organizes and summarizes the observed data. Examples include tables, graphs, and numerical summaries such as means and medians.\nInferential statistics (statistical inference) uses data to learn about something you did not fully observe. Examples include estimating an unknown parameter, predicting a future outcome, or comparing groups while quantifying uncertainty. When doing statistical inference we usually build a probability model to account for data variability, and quantify uncertainty. We assume the sample data are realizations of values generated from that model. Therefore, the model works as a data-generating process or mechanism.\n\n\n\n\n\n\n\n\n\nFigure 1.4: Two broad goals in statistics: describing data and learning from data.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics as a Science of Data</span>"
    ]
  },
  {
    "objectID": "part1-01-stats.html#model-based-statistical-thinking",
    "href": "part1-01-stats.html#model-based-statistical-thinking",
    "title": "1  Statistics as a Science of Data",
    "section": "1.7 Model-based statistical thinking",
    "text": "1.7 Model-based statistical thinking\nModel-based statistics starts from a basic idea.\n\nData vary. Even repeated measurements under similar conditions do not produce identical values.\nWe represent that variation with probability. We treat data as outcomes of random variables and write down a probability model.\nWe learn about unknowns through the model. Unknown quantities are usually model parameters (such as a mean difference, a rate, or a regression slope).\n\nA statistical model is a set of assumptions that connects:\n\nthe data we observe,\nthe chance variation we expect, and\nthe unknown quantities we want to learn about.\n\n\n1.7.1 A model-based workflow\nIn this book, we will repeatedly follow a workflow like this.\n\nState the question in plain language.\nIdentify the data and variables (and how they were generated).\nPropose a probability model for the data generating process.\nFit the model (estimate unknown parameters).\nCheck the model (does it capture important patterns and variability?).\nDraw conclusions with uncertainty clearly communicated.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics as a Science of Data</span>"
    ]
  },
  {
    "objectID": "part1-01-stats.html#frequentist-and-bayesian-paradigms",
    "href": "part1-01-stats.html#frequentist-and-bayesian-paradigms",
    "title": "1  Statistics as a Science of Data",
    "section": "1.8 Frequentist and Bayesian paradigms",
    "text": "1.8 Frequentist and Bayesian paradigms\nModel-based inference can be done in both frequentist and Bayesian ways. The two paradigms share a common starting point:\n\na probability model for the data (often expressed through a likelihood)\n\nThey differ mainly in how they treat unknown parameters and how they interpret probability statements.\n\n1.8.1 Frequentist inference\nIn a frequentist approach:\n\nthe parameter is treated as a fixed (but unknown) constant\nuncertainty is described by what would happen under repeated sampling\n\nTypical outputs include point estimates, confidence intervals, and hypothesis tests.\n\n\n1.8.2 Bayesian inference\nIn a Bayesian approach:\n\nthe parameter is treated as an unknown quantity with a probability distribution\nwe update prior beliefs using the data to obtain a posterior distribution\n\nTypical outputs include posterior means or medians, credible intervals, and posterior probabilities of scientific statements.\n\n\n\n\n\n\nTipHow this book will connect them\n\n\n\nYou will see frequentist and Bayesian tools side by side, using the same underlying probability models whenever possible. The goal is to build one coherent story:\n\nprobability models describe variation\ninference is parameter learning from data\ndifferent paradigms provide different summaries of uncertainty",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics as a Science of Data</span>"
    ]
  },
  {
    "objectID": "part1-01-stats.html#where-we-are-going-next",
    "href": "part1-01-stats.html#where-we-are-going-next",
    "title": "1  Statistics as a Science of Data",
    "section": "1.9 Where we are going next",
    "text": "1.9 Where we are going next\nThis chapter sets the stage: statistics is a science of data because it turns data into knowledge while managing uncertainty.\nIn the next chapters of Part 1, you will:\n\nlearn how data are produced and how design affects conclusions\npractice describing single variables and relationships between variables\nintroduce probability as a language for modeling data\ndevelop the logic of inference through model-based thinking in both frequentist and Bayesian forms",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics as a Science of Data</span>"
    ]
  },
  {
    "objectID": "part1-01-stats.html#footnotes",
    "href": "part1-01-stats.html#footnotes",
    "title": "1  Statistics as a Science of Data",
    "section": "",
    "text": "American Statistical Association, ASA Newsroom. “Statistics is the science of learning from data and of measuring, controlling, and communicating uncertainty.” https://www.amstat.org/asa-newsroom↩︎\nCobb, G. W., and Moore, D. S. (1997). “Mathematics, Statistics, and Teaching.” The American Mathematical Monthly, 104(9), 801–823. The quote “Data are numbers with a context” is widely attributed to this paper. See, for example, the CRAN statquotes vignette: https://cran.r-project.org/web/packages/statquotes/vignettes/statquotes.html↩︎\nNational Academies of Sciences, Engineering, and Medicine (2018). Data Science for Undergraduates: Opportunities and Options. Washington, DC: The National Academies Press. https://nap.nationalacademies.org/catalog/25104/data-science-for-undergraduates-opportunities-and-options↩︎",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics as a Science of Data</span>"
    ]
  },
  {
    "objectID": "part1-01-stats.html#how-are-data-generated-put-in-next-chapter",
    "href": "part1-01-stats.html#how-are-data-generated-put-in-next-chapter",
    "title": "1  Statistics as a Science of Data",
    "section": "1.4 How are data generated? [PUT IN NEXT CHAPTER]",
    "text": "1.4 How are data generated? [PUT IN NEXT CHAPTER]\nData are produced by a process, and that process determines what kinds of conclusions are reasonable.\nCommon data sources include:\n\nSurveys and questionnaires We ask questions and record responses. Survey design affects bias and variability.\nObservational studies We record variables as they naturally occur. Observational data can reveal association but do not automatically justify cause and effect claims.\nExperiments We actively assign a treatment or condition. Random assignment is the key tool for supporting causal conclusions.\nAdministrative and digital records These include logs, transactions, medical records, sensors, and digital traces. These datasets can be large, but they still require careful thinking about measurement and selection.\n\nA useful phrase is data generating process: the real world mechanism that produces the values we observe. In a model-based approach, we build a probability model as an approximation to that process.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics as a Science of Data</span>"
    ]
  },
  {
    "objectID": "part1-01-stats.html#model-based-statistical-thinking-chapter-7",
    "href": "part1-01-stats.html#model-based-statistical-thinking-chapter-7",
    "title": "1  Statistics as a Science of Data",
    "section": "1.7 Model-based statistical thinking [CHAPTER 7]",
    "text": "1.7 Model-based statistical thinking [CHAPTER 7]\nModel-based statistics starts from a basic idea.\n\nData vary. Even repeated measurements under similar conditions do not produce identical values.\nWe represent that variation with probability. We treat data as outcomes of random variables and write down a probability model.\nWe learn about unknowns through the model. Unknown quantities are usually model parameters (such as a mean difference, a rate, or a regression slope).\n\nA statistical model is a set of assumptions that connects:\n\nthe data we observe,\nthe chance variation we expect, and\nthe unknown quantities we want to learn about.\n\n\n1.7.1 A model-based workflow\nIn this book, we will repeatedly follow a workflow like this.\n\nState the question in plain language.\nIdentify the data and variables (and how they were generated).\nPropose a probability model for the data generating process.\nFit the model (estimate unknown parameters).\nCheck the model (does it capture important patterns and variability?).\nDraw conclusions with uncertainty clearly communicated.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics as a Science of Data</span>"
    ]
  },
  {
    "objectID": "part1-01-stats.html#frequentist-and-bayesian-paradigms-chapter-8",
    "href": "part1-01-stats.html#frequentist-and-bayesian-paradigms-chapter-8",
    "title": "1  Statistics as a Science of Data",
    "section": "1.8 Frequentist and Bayesian paradigms [CHAPTER 8]",
    "text": "1.8 Frequentist and Bayesian paradigms [CHAPTER 8]\nModel-based inference can be done in both frequentist and Bayesian ways. The two paradigms share a common starting point:\n\na probability model for the data (often expressed through a likelihood)\n\nThey differ mainly in how they treat unknown parameters and how they interpret probability statements.\n\n1.8.1 Frequentist inference\nIn a frequentist approach:\n\nthe parameter is treated as a fixed (but unknown) constant\nuncertainty is described by what would happen under repeated sampling\n\nTypical outputs include point estimates, confidence intervals, and hypothesis tests.\n\n\n1.8.2 Bayesian inference\nIn a Bayesian approach:\n\nthe parameter is treated as an unknown quantity with a probability distribution\nwe update prior beliefs using the data to obtain a posterior distribution\n\nTypical outputs include posterior means or medians, credible intervals, and posterior probabilities of scientific statements.\n\n\n\n\n\n\nTipHow this book will connect them\n\n\n\nYou will see frequentist and Bayesian tools side by side, using the same underlying probability models whenever possible. The goal is to build one coherent story:\n\nprobability models describe variation\ninference is parameter learning from data\ndifferent paradigms provide different summaries of uncertainty",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistics as a Science of Data</span>"
    ]
  },
  {
    "objectID": "part1-02-data.html#population-and-sample",
    "href": "part1-02-data.html#population-and-sample",
    "title": "2  Data, Studies, and Causality",
    "section": "2.2 Population and Sample",
    "text": "2.2 Population and Sample\n Target Population \nThe data set we collect for analysis is usually a sample of some target population in the study. The first step in conducting a study is to identify questions to be investigated. A clear research question is helpful in identifying\n\nwhat cases should be studied (row)\nwhat variables are important (column)\n\nOnce the research question is determined, it is important to identify the target population to be investigated. The target population is the complete collection of data we’d like to make inference about. Look at the following examples.\n GPA Example \n\n\nSuppose a Marquette professor has a research question: What is the average GPA of currently enrolled Marquette undergraduate students?\nThe target population in this study is  All Marquette undergraduate students that are currently enrolled. because all Marquette undergrads that are currently enrolled are the complete collection of data we’d like to make inference about or we are interested in some property or characteristic of these group of people. Each currently enrolled Marquette undergraduate student is an object in the population. Average GPA is the variable or population property we would like to make an inference about.\n\n\n\n\n\n\nSource: Upslash-Sarah Noltner\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nStudents who are not currently enrolled or students that have already graduated are not our interest, so they shouldn’t be a part of target population.\n\n\n Heart Disease Example \n\n\nDoes a new drug reduce mortality in patients with severe heart disease? If this is our research question, the target population is  All people with severe heart disease.  Mortality is the variable or population property we would like to make an inference about.\nDo you think it is possible the apply the new drug to all the patients with severe heart disease, and obtain the mortality we are interested?\n\n\n\n\n\n\nSource: Upslash-National Cancer Institute\n\n\n\n\n\n\n\n Sample Data \nIn some cases it’s possible to collect data of all the cases we are interested in. However, most of the time it is either too expensive or too time consuming to collect data for every case in a population. What if we tried to collect data on the average GPA of all students in Illinois? The U.S.? The world? 😱 😱 😱\n\n\nWhen we are not able to collect all the cases in the target population due to some budget or time constraint, but we still want to learn about the population property, our solution is sampling cases from the target population. A sample is a subset of cases selected from a population.\nWe are not able to collect the average GPA of every member of the population, but we can collect a sample from that population which has fewer objects (Figure 2.2). We can then compute the average GPA of the sample data. \n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: Sampling from the population reduces the number of objects from which to collect data.\n\n\n\n\n\n\n\nWe hope that the average GPA of the sample is close to the average GPA of the population, which is our main interest. For the sample’s average GPA to be close to population’s average GPA, we want the sample to look like the population such that they share similar attributes including GPA. In other words, we hope the sample is a small size of everything in the population, and the sample is representative of the population. Ideally, a sample is the small size of some dish and the population is the large size of that dish.\n\n Good Sample vs. Bad Sample \n\n\n\n\n\n\nNoteIs this 4720/5720 class a sample of the target population Marquette students?\n\n\n\n\n\n\nOf course, every member in the class is a Marquette student, so the class is a subset of the population Marquette students. Let me ask you another question.\n\n\n\n\n\n\nNoteIs this 4720/5720 class a “good” sample of the target population?\n\n\n\n\n\n\nRemember a good sample should well represent the target population. Do you think the class is generally a small version of the entire Marquette student body? The sample is convenient to be collected, but as Figure 2.3 shows, it is NOT representative of the population. Because this class is primarily composed of STEM majors, it may not share the attributes necessary with the target population for the two to share a similar average GPA. We call this kind of sample a biased sample. The average GPA of the class may differ greatly from the average GPA of all Marquette undergrads.\n\n\n\n\n\n\n\n\nFigure 2.3: Majors of students in this 4720/5720 class\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.4: Sampling from a class of mostly STEM students is not representative of the entire population.\n\n\n\n\n\n\n\nIf the biased sample has no or tiny impact on the population attribute we’d like to discover, we are lucky, and the issue is not serious. However, if the sample is biased in a way that the attribute we get from the sample is quite different from that of the population, then we miss the mark. In the GPA example, if the average GPA depends on students major, the sample that is biased in students’ major causes a trouble.\nAs shown in Figure 2.5, the average GPA differs based on students’ majors. Because this class consists of mostly STEM majors, it is likely that the average GPA of its students is not the same as the average GPA of all Marquette undergraduates. Figure 2.4 depicts that sampling needs to be done appropriately to ensure the sample is representative of the population. \n\n\n\n\n\n\n\n\n\n\n\nFigure 2.5: UC Berkeley average GPAs by major\n\n\n\n\n\n\n\n\n\nHow do we collect a representative sample? We always seek to randomly select a sample from a population. Every member in the target population should be treated equally, and preferably has equal chance to be chosen in the sample. If you just collect data from STEM fields, we miss the information provided by students from arts, humanities, and other non-STEM majors. Random sampling usually give us a representative sample, as long as the sample size, the number of objects in the sample, is not too small. It is important to collect samples this way because many statistical methods are based on the random sampling assumption.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data, Studies, and Causality</span>"
    ]
  },
  {
    "objectID": "part1-02-data.html#data-collection",
    "href": "part1-02-data.html#data-collection",
    "title": "2  Data, Studies, and Causality",
    "section": "2.3 Data Collection",
    "text": "2.3 Data Collection\nIn this section, we briefly discuss how data can be collected.\n Two Types of Studies to Collect Sample Data \nThere are two types of studies that are used to collect data: observational studies and experimental studies.\nAn observational study is a study in which those collecting the data observe and measure characteristics/variables, but do NOT attempt to modify or intervene with the subjects being studied.\n\n Example: Sample from 1️⃣ the heart disease and 2️⃣ heart disease-free populations and record the fat content of the diets for the two groups.  In this type of study, the researchers do not modify or intervene with the the subjects either with or without heart disease. They just record the fat content in their diets.\n\nThe other type of study is called the experimental study. In an experimental study, some treatment(s) is applied and then those collecting data proceed to observe its responses or effects on the individuals, the experimental units in such study.\n\n Example: Assign volunteers to one of several diets with different levels of dietary fat and compare the fat levels with respect to the incidence of heart disease after a period of time.  In this experimental study, the treatment is the fat level in diets. The researchers do not just observe the subjects behavior. Instead, they ask the subjects to take a specific diet they design for a period of time.\n\n\n\n\n\n\n\nNoteObservational or Experimental?\n\n\n\n\nRandomly select 40 males and 40 females to see the difference in blood pressure levels between male and female.\nTest the effects of a new drug by randomly dividing patients into 3 groups (high dosage, low dosage, placebo).\n\n\n\n Limitation of Observational Studies: Confounding Variables \nOne limitation of observational studies is confounding. A confounder is a variable that is not included in a study, but affects the variables in the study. For example, a person observes past data showing that increases in ice cream sales are associated with increases in drownings and concludes that eating ice cream causes drownings. 😱😕⁉️\n\n\n\n\n\n\n\nSource: Unsplash-Brooke Lark\n\n\n\n\n\n\n\n\n\n\n\n\nSource: Unsplash-Nate Neelson\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhat is the confounder that is not in the data but affects ice cream sales and the number of drownings?\n\n\n\n\nTemperature\n\n\n\n\nAs temperature increases, ice cream sales increase and the number of drownings also rises because more people go swimming (Figure 2.6).\n\n\n\n\n\n\n\n\n\nFigure 2.6: Temperature acting as a confounder\n\n\n\n\n\n\nMaking causal conclusions based on experimental data is often more reasonable than making the same causal conclusions based on observational data. Observational studies are generally only sufficient to show associations, not causality.\n\n Types of Random Samples \nAs previously mentioned, many statistical methods are based on the randomness assumption. It’s important to understand what a random sample is and how to collect it. In a random sample, each member of a population is equally likely to be selected.\n Simple Random Sample \nFor a simple random sample (SRS), every possible sample of sample size \\(n\\) has the same chance to be chosen.\n\nExample: If I were to sample 100 students from all 10,000 Marquette students, I would randomly assign each student a number (from 1 to 10,000) and then randomly select 100 numbers.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.7: Simple Random Sample\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.8: Simple random sample from a population of 15 (https://research-methodology.net/sampling-in-primary-data-collection/random-sampling/)\n\n\n\n\n\n\n\n Stratified Random Sample \nFor stratified sampling, we subdivide the population into different subgroups (strata) that share the same characteristics, then draw a simple random sample from each subgroup. Stratified sampling has a property: Homogeneous within strata; Non-homogeneous between strata. (Figure 2.9)\n\n\n\n\n\n\n\n\nFigure 2.9: Stratified Sampling. Source: https://www.datasciencemadesimple.com/stratified-random-sampling-in-r-dataframe-2/\n\n\n\n\n\n\nExample: Divide Marquette students into groups by colleges, then perform a SRS for each group (Figure 2.10). In this case, subjects within strata are homogeneous because people in the same stratum belong to the same college. Subjects are non-homogeneous between strata because students in one college is not a student in another college.\n\n\n\n\n\n\n\n\n\nFigure 2.10: Stratified sampling of Marquette Students\n\n\n\n\n\n Cluster Sampling \nFor cluster sampling, divide the population into clusters, then randomly select some of those clusters, and then keep all the members from those selected clusters. Cluster sampling has a property: Homogeneous between clusters; Non-homogeneous within clusters (Figure 2.11). Clusters look similar each other, but members in a cluster are not very alike. They have different characteristics.\n\n\n\n\n\n\n\n\nFigure 2.11: Cluster Sampling Source: https://research-methodology.net/sampling-in-primary-data-collection/cluster-sampling/\n\n\n\n\n\n\nExample: Study 4720 students’ drinking habits by dividing the students into 9 groups, and then randomly selecting 3 and interviewing all of the students in each of those clusters (Figure 2.12). Subjects are homogeneous between clusters because clusters are like random partitions, and each one is a representative subset of the entire population. Subjects are non-homogeneous within clusters because everyone has their own characteristics, and subjects are not divided based on any characteristic such as major or college.\n\n\n\n\n\n\n\n\n\nFigure 2.12: Cluster sampling of Marquette students",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data, Studies, and Causality</span>"
    ]
  },
  {
    "objectID": "part1-02-data.html#data-type",
    "href": "part1-02-data.html#data-type",
    "title": "2  Data, Studies, and Causality",
    "section": "2.4 Data Type",
    "text": "2.4 Data Type\nOK. We learn data collection and sampling methods. Now’s let’s learn some data types. Usually a statistical method is only for some type of data or variables. Knowing data types is important because it helps us choose the correct or appropriate statistical methods for analysis. It also helps us interprets the analysis result correctly. Figure 2.13 tells us everything about data type. We are going to learn each data type in the figure.\n\n\n\n\n\n\n\n\nFigure 2.13: Types of Data\n\n\n\n\n\n Categorical vs. Numerical Variables \nA categorical variable provides non-numerical information which can be placed in one (and only one) category from two or more categories. Here are some examples.\n\nGender (Male 👨, Female 👩, Trans 🏳️‍🌈) \nClass (Freshman, Sophomore, Junior, Senior, Graduate) \nCountry (USA 🇺🇸, Canada 🇨🇦, UK 🇬🇧, Germany 🇩🇪, Japan 🇯🇵, Korea 🇰🇷) \n\nGender, Class, and Country are all categorical variables because they provide non-numerical information. Their possible “values” are “categories”. Keep in mind that a data object can only belong to one category of that variable. You cannot be a freshman and sophomore.\nA numerical variable is recorded in a numerical value representing counts or measurements. Some examples are\n\n GPA \n The number of relationships you’ve had \n Height \n\nThe possible values of the three variables are all numerical or numbers. You are a 6’2” tall student who had eight girlfriends and your GPA is 3.98.\n Numerical Variables \nNumerical variables can be discrete or continuous. A discrete variable takes on values of a finite or countable number, while a continuous variable takes on values anywhere over a particular range without gaps or jumps.\n\n GPA is continuous because theoretically it can be any value between 0 and 4. \n The number of relationships you’ve had is discrete because you can count the number and it is finite. The possible values are 0, 1, 2, 3, and so on. Can you have a 0.5 relationship?\n Height is continuous because it can be any number within a range. \n\n Categorical Variables \nFor convenience, categorical variables are usually recorded as numbers in a data set. For example, we can have\n\nGender (Male = 0, Female = 1, Trans = 2) \nClass (Freshman = 1, Sophomore = 2, Junior = 3, Senior = 4, Graduate = 5) \nCountry (USA = 100, Canada = 101, UK = 200, Germany = 201, Japan = 300, Korea = 301) \n\nEven United Airlines boarding group is categorical. The group number does provide non-numerical information, which is the order of boarding. You cannot be in both boarding zone one and zone two for the same ticket. You can only be in one group.\nPlease note that the numbers represent categories only; taking differences of these numbers is meaningless. If we use the coding scheme in the examples,\n\nCanada - USA = 101 - 100 = 1???\nGraduate - Sophomore = 5 - 2 = 3 = Junior???\n\nThe arithmetic operations do not make sense. For any data or variables, we need to learn the level of measurements to know which arithmetic operations are meaningful for what type of data.\n\n Levels of Measurements \n Nominal and Ordinal for Categorical Variables \nA categorical variable can be of nominal or ordinal level of measurement.\nThe data is nominal if can not be ordered in a meaningful or natural way. For example,\n\nGender (Male = 0, Female = 1, Trans = 2)  is nominal because Male, Female and Trans cannot be ordered, even the numbering coding has an ordering.\nCountry (USA = 100, Canada = 101, UK = 200, Germany = 201, Japan = 300, Korea = 301)  is nominal. There is no reason to put any country before any other country unless there is another variable giving those countries another attribute that can be ordered.\n\nOrdinal data can be arranged in some meaningful order, but differences between data values can NOT be determined or are meaningless.\n\nClass (Freshman = 1, Sophomore = 2, Junior = 3, Senior = 4, Graduate = 5)  is ordinal because Sophomore is one class higher than Freshman, and so on. Here the difference is still meaningless. It seems that Junior is one year higher than Sophomore, and Junior - Sophomore = 1 kind of makes sense. However, “1” does not mean one year higher; instead “1” means Freshman. Moreover, we could even use the numbering (Freshman = 1, Sophomore = 10, Junior = 33, Senior = 44, Graduate = 50) for the Class variable.\n\n Interval and Ratio for Numerical Variables \nNumerical data can be interval or ratio level of measurement.\nInterval data have meaningful differences between any two values but the data do NOT have a natural zero or starting point. The data can do \\(\\color{red} +\\) and \\(\\color{red} -\\), but can’t reasonably do \\(\\color{red} \\times\\) and \\(\\color{red} \\div\\).\n\nTemperature is interval because \\(80^{\\circ}\\)F is 40 degrees higher than \\(40^{\\circ}\\)F \\((80-40=40)\\), but \\(0^{\\circ}\\) does not mean NO heat or NO temperature, but a specific temperature. Also, \\(80^{\\circ}\\)F is NOT twice as hot as \\(40^{\\circ}\\)F.\n\nRatio data have both meaningful differences and ratios, and there is a natural zero starting point that indicates none of the quantity. The data can do \\(\\color{red} +\\), \\(\\color{red} -\\), \\(\\color{red} \\times\\) and \\(\\color{red} \\div\\).\n\nDistance is ratio level of measurement because \\(80\\) miles is twice as far as \\(40\\) miles \\((80/40 = 2)\\), and \\(0\\) mile means NO distance.\n\n\n Converting Numerical to Categorical \nSometimes research purpose we may want to convert a numerical variable into a categorical variable. Figure 2.14 is an example of turning a 100% percentage grade into a letter grade which is categorical. Another is example is turning annual salary (numerical) into income level (categorical). We can say salary between $0 and $50,000 is “low” income level, salary between $50,000 and $120,000 is “middle” income level, and above $120,000 is “high” income level.\n\n\n\n\n\n\n\n\nGrade\n\n\nPercentage\n\n\n\n\n\n\nA\n\n\n[94, 100]\n\n\n\n\nA-\n\n\n[90, 94)\n\n\n\n\nB+\n\n\n[87, 90)\n\n\n\n\nB\n\n\n[83, 87)\n\n\n\n\nB-\n\n\n[80, 83)\n\n\n\n\nC+\n\n\n[77, 80)\n\n\n\n\nC\n\n\n[73, 77)\n\n\n\n\nC-\n\n\n[70, 73)\n\n\n\n\nD+\n\n\n[65, 70)\n\n\n\n\nD\n\n\n[60, 65)\n\n\n\n\nF\n\n\n[0, 60)\n\n\n\n\n\n\nFigure 2.14: Grading scale for this class\n\n\n\n\n\n Practice \n\n\n\n\n\n\nWarningYour turn!\n\n\n\nIdentify the data type of each variable in the Marquette men’s basketball player data\n\n\n\n\n\n\n\n\n\n\nFigure 2.15: 2019 Marquette men’s basketball player data set.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data, Studies, and Causality</span>"
    ]
  },
  {
    "objectID": "part1-02-data.html#exercises",
    "href": "part1-02-data.html#exercises",
    "title": "2  Data, Studies, and Causality",
    "section": "2.5 Exercises",
    "text": "2.5 Exercises\n\nData Type: Identify each of the following as numerical or categorical data.\n\nThe names of the companies that manufacture paper towels\nThe colors of cars\nThe heights of football players\n\nLevel of Measurements: Identify the level of measurement used in each of the following.\n\nThe weights of people in a sample of people living in Milwaukee.\nA physician’s descriptions of “abstains from smoking, light smoker, moderate smoker, heavy smoker.”\nFlower classifications of “rose, tulip, daisy.”\nSuzy measures time in days, with 0 corresponding to her birth date. The day before her birth is -1, the day after her birth is +1, and so on. Suzy has converted the dates of major historical events to her numbering system. What is the level of measurement of these numbers?\n\nDiscrete vs Continuous: Determine whether the data are discrete or continuous.\n\nThe length of stay (in days) for each COVID patient in Wisconsin.\nSeveral subjects are randomly selected and their heights are recorded.\nFrom a data set, we see that a male had an arm circumference of 31.28 cm.\nA sample of married couples is randomly selected and the number of animals in each family is recorded.\n\nSampling Method: Identify which of these types of sampling is used: random, stratified, or cluster.\n\nDr. Yu surveys his statistics class by identifying groups of males and females, then randomly selecting 7 students from each of those two groups.\nDr. Yu conducts a survey by randomly selecting 5 different sports teams at Marquette and surveying all of the student-athletes on those teams.\n427 subjects were randomly assigned to (1) meditation or (2) no mediation group to study the effectiveness of this mindfulness activity on lowering blood pressure.\n\nStudy Type: Determine whether the study is an experiment or an observational study, then identify a major problem with this study.\n\nIn a survey conducted by USA Today, 998 Internet users chose to respond to the question:“How often do you seek medical advice online?” 42% of the respondents said “frequently.”\nThe Physicians’ Health Study involved 21,045 female physicians. Based on random selections, 11,224 of them were treated with aspirin and other other 9,821 were given placebos. The study was stopped early because it became clear that aspirin did not reduce the risk of myocardial infarctions by a substantial amount.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data, Studies, and Causality</span>"
    ]
  },
  {
    "objectID": "part1-02-data.html#what-data-mean-in-statistics",
    "href": "part1-02-data.html#what-data-mean-in-statistics",
    "title": "2  Data: Meaning, Collection, and Types",
    "section": "",
    "text": "Data are not only the values. Data also include context: who or what was measured, when and where the measurement happened, and how the measurement was taken.\nData come from a process. Even if the same procedure is repeated, the recorded values will typically vary. Statistics treats that variability as a central feature, not as an inconvenience.\n\n\n2.1.1 Some key terms\nObservational unit (or case). The object being described by a row of data. In the coffee shop story, an observational unit might be a single order, a single student visit, or a single hour, depending on how the data set is defined.\nVariable. A characteristic recorded for each observational unit. In statistics, we often treat a variable as something that can vary across units and across repetitions of data collection.\nData set. A collection of variables measured on a collection of observational units, together with documentation that explains what each variable means and how it was recorded.\nData dictionary (or codebook). A document that defines each variable, its units, allowed values, and any special codes for missing or unknown values.\n\n\n\n\n\n\nTipWhy this matters for model-based statistics\n\n\n\nModel-based statistics starts from a simple idea: the data you see are one possible outcome of a data generating process. Later in the book, we will describe that process using probability models. In this chapter, we stay at the level of careful description, because a probability model cannot rescue a poorly defined variable or a poorly collected data set.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data: Meaning, Collection, and Types</span>"
    ]
  },
  {
    "objectID": "part1-02-data.html#variables-and-data-sets",
    "href": "part1-02-data.html#variables-and-data-sets",
    "title": "2  Data: Meaning, Collection, and Types",
    "section": "2.2 Variables and data sets",
    "text": "2.2 Variables and data sets\nMost data sets you will work with in this book have this structure.\n\nEach row corresponds to an observational unit\nEach column corresponds to a variable\nEach cell is the recorded value of one variable for one unit\n\nThis structure is often called a tidy or rectangular data set. It is not the only possible format, but it is the most common starting point for statistical work in R.\n\n2.2.1 Example: a small data set from the coffee shop story\nThe following table is a toy example. It is small on purpose so we can talk about meaning, collection, and type.\n\ncoffee &lt;- data.frame(\n  order_id     = 1:12,\n  time_block   = c(\"8 to 9\", \"8 to 9\", \"8 to 9\", \"9 to 10\", \"9 to 10\", \"9 to 10\",\n                   \"10 to 11\", \"10 to 11\", \"10 to 11\", \"11 to 12\", \"11 to 12\", \"11 to 12\"),\n  mobile_order = c(\"yes\",\"no\",\"no\",\"yes\",\"no\",\"yes\",\"no\",\"no\",\"yes\",\"yes\",\"no\",\"no\"),\n  wait_min     = c(4, 11, 9, 5, 10, 6, 8, 7, 5, 4, 6, 7),\n  items        = c(1, 2, 1, 1, 3, 2, 2, 1, 1, 1, 2, 1),\n  rating       = c(5, 2, 3, 4, 2, 4, 3, 3, 4, 5, 4, 3)\n)\n\nknitr::kable(coffee, caption = \"Toy example data from the coffee shop story.\")\n\n\n\nTable 2.1: Toy example data from the coffee shop story.\n\n\n\n\n\n\norder_id\ntime_block\nmobile_order\nwait_min\nitems\nrating\n\n\n\n\n1\n8 to 9\nyes\n4\n1\n5\n\n\n2\n8 to 9\nno\n11\n2\n2\n\n\n3\n8 to 9\nno\n9\n1\n3\n\n\n4\n9 to 10\nyes\n5\n1\n4\n\n\n5\n9 to 10\nno\n10\n3\n2\n\n\n6\n9 to 10\nyes\n6\n2\n4\n\n\n7\n10 to 11\nno\n8\n2\n3\n\n\n8\n10 to 11\nno\n7\n1\n3\n\n\n9\n10 to 11\nyes\n5\n1\n4\n\n\n10\n11 to 12\nyes\n4\n1\n5\n\n\n11\n11 to 12\nno\n6\n2\n4\n\n\n12\n11 to 12\nno\n7\n1\n3\n\n\n\n\n\n\n\n\nEven this small table raises questions you must answer before analysis.\n\nWhat is the observational unit: an order, a student visit, or a person\nWhat is the meaning of wait time: from payment to pickup, or from joining the line to pickup\nHow was rating collected: asked immediately, or later by email, or only for app users\nAre mobile orders counted differently from in person orders\n\nThose questions are not technicalities. They determine what conclusions are credible.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data: Meaning, Collection, and Types</span>"
    ]
  },
  {
    "objectID": "part1-02-data.html#how-data-are-generated",
    "href": "part1-02-data.html#how-data-are-generated",
    "title": "2  Data: Meaning, Collection, and Types",
    "section": "2.3 How data are generated",
    "text": "2.3 How data are generated\nA useful way to think about data generation is to separate three layers.\n\nThe real world process (people, behavior, systems)\nThe measurement process (what gets recorded, with what instrument, with what rules)\nThe data set you analyze (rows, columns, codes, and missing values)\n\nA problem can enter at any layer. For example, a campus survey might have a well written question but a poor response rate, or it might have a high response rate but a confusing question.\n\n2.3.1 Population and sample\nA population is the larger group you want to learn about. A sample is the group you actually observe.\nIn the coffee shop story, several populations are possible.\n\nAll coffee shop visits on campus this semester\nAll student visits during weekday mornings\nAll students who buy coffee on campus\n\nThe population should be defined by the question, not by convenience. The sample is determined by the collection method and practical constraints.\n\n\n2.3.2 Representation and bias\nA sample is most useful when it represents the population for the question being asked. Two common reasons a sample does not represent the population are:\n\nSelection bias. Some units are more likely to be included than others, in a way related to the variables of interest.\nNonresponse or missingness. Some units are included, but some variables are not recorded, in a way related to the outcomes.\n\nIn model-based work, we often describe selection and missingness as parts of the data generating process. If they are severe, they can dominate any later modeling step.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data: Meaning, Collection, and Types</span>"
    ]
  },
  {
    "objectID": "part1-02-data.html#how-data-are-collected",
    "href": "part1-02-data.html#how-data-are-collected",
    "title": "2  Data: Meaning, Collection, and Types",
    "section": "2.4 How data are collected",
    "text": "2.4 How data are collected\nMost introductory examples fall into one of these collection approaches.\n\n2.4.1 Sample surveys\nA sample survey collects data from a subset of a population, typically using a questionnaire or structured measurement plan.\nKey design ideas include:\n\nA sampling frame that defines who can be selected\nA selection method, ideally involving randomization\nClear measurement definitions and units\nA plan for handling nonresponse\n\n\n\n2.4.2 Observational studies\nAn observational study records variables as they naturally occur, without assigning treatments or interventions. Observational studies are common in social science, public health, business analytics, and campus life.\nIn the coffee shop story, using transaction logs to compare wait times for mobile versus non mobile orders is observational unless orders were assigned by design. Students choose whether to use the app, and that choice may be related to other factors such as schedule or patience.\n\n\n2.4.3 Experiments\nAn experiment assigns an intervention to units, such as a treatment, policy, or feature change, and then measures outcomes. Random assignment is a powerful tool because it helps separate the intervention effect from other differences between groups.\nIn a campus setting, a small experiment might randomly assign some time blocks to run the mobile ordering pilot, while keeping staffing levels comparable. Then the comparison of wait times has a clearer causal interpretation.\n\n\n\n\n\n\nWarningA practical warning\n\n\n\nMany real data sets combine collection approaches. A data set might include administrative records plus a survey, or sensor data plus a short experiment. When sources are combined, documentation becomes even more important.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data: Meaning, Collection, and Types</span>"
    ]
  },
  {
    "objectID": "part1-02-data.html#where-data-come-from",
    "href": "part1-02-data.html#where-data-come-from",
    "title": "2  Data: Meaning, Collection, and Types",
    "section": "2.5 Where data come from",
    "text": "2.5 Where data come from\nModern data work often begins by asking not only how the data were collected, but where the data originate. Here are common sources.\n\n2.5.1 Primary collection\nYou or your team collect the data for a specific question, such as a class survey or a designed experiment. Primary collection offers control, but it requires effort and planning.\n\n\n2.5.2 Administrative and transaction records\nThese are records created as part of running an organization: course registrations, dining swipes, library checkouts, or purchases. They can be large and detailed, but the variables were not originally designed for your research question.\n\n\n2.5.3 Digital traces and platform logs\nApps and websites generate timestamped logs: clicks, views, location pings, and usage patterns. These data can be informative, but they raise privacy and consent issues and they often require careful cleaning.\n\n\n2.5.4 Sensors and instruments\nWearables, lab instruments, and environmental sensors produce streams of measurements. These data can be high frequency and high volume, which makes data type and data structure especially important.\n\n\n2.5.5 Public and open data\nGovernments, research groups, and organizations release public data sets. Open data can be valuable for learning and research, but you still must understand the original collection process and limitations.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data: Meaning, Collection, and Types</span>"
    ]
  },
  {
    "objectID": "part1-02-data.html#data-types",
    "href": "part1-02-data.html#data-types",
    "title": "2  Data: Meaning, Collection, and Types",
    "section": "2.6 Data types",
    "text": "2.6 Data types\nData type is about what values a variable can take and what operations make sense on those values. Data type shapes:\n\nHow you summarize a variable in Chapter 3\nWhat kinds of relationships you look for in Chapter 4\nWhat probability models are reasonable later in the book\n\n\n2.6.1 Two broad families\nCategorical variables. Values are labels for categories.\n\nNominal categorical: categories have no natural order (major, residence hall)\nOrdinal categorical: categories have a natural order (rating from 1 to 5)\n\nQuantitative variables. Values are numbers where arithmetic meaning is appropriate.\n\nDiscrete quantitative: counts (number of items ordered)\nContinuous quantitative: measurements on a scale (wait time in minutes)\n\n\n\n2.6.2 Common data types and model building blocks\nLater chapters will introduce probability models carefully. For now, it is enough to see the connection between variable type and the kind of randomness we might model.\n\n\n\n\n\n\n\n\nVariable type\nExample\nA common probability model family\n\n\n\n\nBinary categorical\nmobile order yes or no\nBernoulli or Binomial\n\n\nNominal categorical\nresidence hall\nCategorical or Multinomial\n\n\nOrdinal categorical\nrating 1 to 5\nOrdered categorical models\n\n\nCount\nitems purchased\nPoisson or Negative Binomial\n\n\nContinuous\nwait time\nNormal, log normal, or other continuous models\n\n\nProportion\nfraction of correct answers\nBinomial or Beta type models\n\n\n\n\n\n2.6.3 Data type depends on decisions\nThe same underlying idea can be recorded in different ways.\n\nA rating of 1 to 5 is ordinal categorical, but you might treat it as quantitative when computing an average. That choice has consequences.\nAge can be recorded as a number, or grouped into categories. Grouping can simplify summaries, but it discards information.\nA continuous measurement can be rounded to an integer, which changes the type and may change the model.\n\nGood practice is to document these decisions and to align them with the goals of the analysis.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data: Meaning, Collection, and Types</span>"
    ]
  },
  {
    "objectID": "part1-02-data.html#practice",
    "href": "part1-02-data.html#practice",
    "title": "2  Data: Meaning, Collection, and Types",
    "section": "2.7 Practice",
    "text": "2.7 Practice\n\n\n\n\n\n\nWarningYour turn\n\n\n\nReturn to the coffee shop story and answer these questions.\n\nWhat is a reasonable observational unit if the goal is to study wait time\nList four variables you would want, and describe how each would be measured\nIdentify which variables are categorical and which are quantitative\nFor one categorical and one quantitative variable, describe a sensible summary you might use in Chapter 3",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data: Meaning, Collection, and Types</span>"
    ]
  },
  {
    "objectID": "part1-02-data.html#chapter-summary",
    "href": "part1-02-data.html#chapter-summary",
    "title": "2  Data: Meaning, Collection, and Types",
    "section": "2.8 Chapter summary",
    "text": "2.8 Chapter summary\nIn this chapter, you learned how statisticians interpret data and why data meaning and data generation matter.\n\nData are recorded values with context and purpose\nA data set has observational units and variables, supported by documentation\nData are generated by a real world process and a measurement process\nCollection approaches include surveys, observational studies, and experiments\nModern data sources include administrative records, digital traces, sensors, and open data\nData types guide both summaries and later probability modeling",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data: Meaning, Collection, and Types</span>"
    ]
  },
  {
    "objectID": "part1-02-data.html#where-we-are-going-next",
    "href": "part1-02-data.html#where-we-are-going-next",
    "title": "2  Data: Meaning, Collection, and Types",
    "section": "2.9 Where we are going next",
    "text": "2.9 Where we are going next\nChapter 3 focuses on summarizing one variable at a time. You will learn how to use graphs and numerical summaries to describe patterns and variability, and how those summaries prepare you for model-based inference later in the book.\n\n\n–&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;  –&gt; \n\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;              \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;  –&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;\n\n–&gt; \n\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;        –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n\n–&gt;  –&gt;  –&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data: Meaning, Collection, and Types</span>"
    ]
  }
]